{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phase 4_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN183M1609VgKHL/be+/O7L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spe301/dsc-phase-4-project/blob/main/Phase_4_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "fWlxIpyXDK7d",
        "outputId": "a0e0e877-3c6c-4272-feb8-7e1a8c11aa79"
      },
      "source": [
        "#Data loading for csv and txt files\n",
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d607caa5-e6ac-43f8-b497-211adf18be74\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d607caa5-e6ac-43f8-b497-211adf18be74\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving judge-1377884607_tweet_product_company.csv to judge-1377884607_tweet_product_company.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thNMHllJDeii"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('judge-1377884607_tweet_product_company.csv', engine='python')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO-Wvb-IEhFT"
      },
      "source": [
        "#dropping row 6 as there's no tweet in that row and dropping all rows where sentiment is \"I can't tell\"\n",
        "df = df.drop(6)\n",
        "df = df.loc[df['is_there_an_emotion_directed_at_a_brand_or_product'] != \"I can't tell\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe0Nsr0WTAeS"
      },
      "source": [
        "#stripping punctuation\r\n",
        "ix = list(df.index)\r\n",
        "clean = []\r\n",
        "for i in range(len(ix)):\r\n",
        "  clean.append(df['tweet_text'][ix[i]].replace('.@', '').replace('.', '').replace('!', '').replace('@', '').replace('?', '').replace(':', '')\r\n",
        "  .replace(',', '').replace(';', '').lower())\r\n",
        "\r\n",
        "df['tweet'] = clean"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFEv1nLqEKUh"
      },
      "source": [
        "These are a some classes of helper functions that will make modeling and eda more efficient.\n",
        "\n",
        "- Data Helper: data preprocessing\n",
        "  - MulticlassOutput: output preprocessing\n",
        "  - ModelReadyText1: converts tweets into tokenized sequences\n",
        "  - ModelReadyText2: converts tweets into  one-hot-encoded vectors\n",
        "  - ViewAccuracy: plots a model's accuracy throughout training\n",
        "  - ViewLoss: plots a model's accuracy throughout training\n",
        "\n",
        "- Deep Learning: model building and testing \n",
        "  - Powers: help the model building functions determine nlayers\n",
        "  - Sparsity: help the model building function know when to use 'sparse_categorical_crossentropy' as loss function\n",
        "  - FeedForward: builds a simple neural net with only Dense Layers\n",
        "  - RNN: builds a neural network with LSTM or GRU layer(s)\n",
        "  - TestDL: wraps the keras wrapper functions and GridSearchCV into a simple one liner, one line plus the parameter grid that is :)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jfMSv1qEjI3"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import models, layers\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from math import log\n",
        "from keras.regularizers import  L2, l1\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "class DataHelper:\n",
        "\n",
        "  def MulticlassOutput(self, df, col):\n",
        "    enc = OneHotEncoder()\n",
        "    y = np.array(df[col]).reshape(-1, 1)\n",
        "    oh = enc.fit_transform(y).toarray()\n",
        "    return oh\n",
        "\n",
        "\n",
        "  def ModelReadyText1(self, df, Xcol, ycol, pad):\n",
        "    text = list(df[Xcol])\n",
        "    t = Tokenizer()\n",
        "    t.fit_on_texts(text)\n",
        "    tokens = t.texts_to_sequences(text)\n",
        "    tokens2 = pad_sequences(tokens, maxlen=pad)\n",
        "    dh = DataHelper()\n",
        "    y = dh.MulticlassOutput(df, ycol)\n",
        "    return tokens2, y\n",
        "\n",
        "  def ModelReadyText2(self, df, Xcol, ycol, num_words):\n",
        "    dh = DataHelper()\n",
        "    text = list(df[Xcol])\n",
        "    t = Tokenizer(num_words=num_words)\n",
        "    t.fit_on_texts(text)\n",
        "    oh = t.texts_to_matrix(text)\n",
        "    y = dh.MulticlassOutput(df, ycol)\n",
        "    return oh, y\n",
        "\n",
        "  def ViewAccuracy(self, history, epochs):\n",
        "    plt.plot(range(epochs), history.history['accuracy'], label='train');\n",
        "    plt.plot(range(epochs), history.history['val_accuracy'], label='val');\n",
        "    plt.legend(loc='best')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('accuracy')\n",
        "    return None\n",
        "\n",
        "\n",
        "  def ViewLoss(self, history, epochs):\n",
        "      plt.plot(range(epochs), history.history['loss'], label='train');\n",
        "      plt.plot(range(epochs), history.history['val_loss'], label='val');\n",
        "      plt.legend(loc='best')\n",
        "      plt.xlabel('epochs')\n",
        "      plt.ylabel('loss')\n",
        "      return None\n",
        "\n",
        "class DeepLearning:\n",
        "\n",
        "  def Powers(self, n):\n",
        "    k = int(log(n, 2))\n",
        "    return k\n",
        "\n",
        "  def Sparsity(self, x):\n",
        "    df = pd.DataFrame(x[0])\n",
        "    df.columns = ['val']\n",
        "    df2 = df.loc[df['val'] == 0]\n",
        "    return len(df)*0.9 <= len(df2)\n",
        "\n",
        "  def FeedForward(self, nodes, activation, optimizer, dropout, regularizer, inp, nclasses):\n",
        "    dl = DeepLearning()\n",
        "    if nclasses > 16:\n",
        "      pen = nclasses*2\n",
        "    else:\n",
        "      pen = 16\n",
        "    if nclasses == 2:\n",
        "      oa = 'sigmoid'\n",
        "      loss = 'binary_crossentropy'\n",
        "    else:\n",
        "      oa = 'softmax'\n",
        "      if dl.Sparsity(X) == False:\n",
        "        loss = 'categorical_crossentropy'\n",
        "      else:\n",
        "        loss = 'sparse_categorical_crossentropy'\n",
        "    model = models.Sequential()\n",
        "    if regularizer == None:\n",
        "      model.add(layers.Dense(nodes, activation=activation, input_shape=(inp,)))\n",
        "    if regularizer == 'L1':\n",
        "      model.add(layers.Dense(nodes, activation=activation, input_shape=(inp,), kernel_regularizer=l1(0.005)))\n",
        "    if regularizer == 'L2':\n",
        "      model.add(layers.Dense(nodes, activation=activation, input_shape=(inp,), kernel_regularizer=L2(0.005)))\n",
        "    if dropout == True:\n",
        "      model.add(layers.Dropout(0.5))\n",
        "    p = dl.Powers(nodes)\n",
        "    for i in range(p):\n",
        "      nodes /= 2\n",
        "      nodes = int(nodes)\n",
        "      if nodes > 16:\n",
        "        if nodes > nclasses:\n",
        "          model.add(layers.Dense(nodes, activation=activation))\n",
        "    model.add(layers.Dense(pen, activation=activation))\n",
        "    model.add(layers.Dense(nclasses, activation=oa))\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  def RNN(self, nodes, activation, optimizer, dropout, regularizer, method, stacking, inp, nclasses):\n",
        "    dl = DeepLearning()\n",
        "    if nclasses > 16:\n",
        "      pen = nclasses*2\n",
        "    else:\n",
        "      pen = 16\n",
        "    if nclasses == 2:\n",
        "      oa = 'sigmoid'\n",
        "      loss = 'binary_crossentropy'\n",
        "    else:\n",
        "      oa = 'softmax'\n",
        "      if dl.Sparsity(X) == False:\n",
        "        loss = 'categorical_crossentropy'\n",
        "      else:\n",
        "        loss = 'sparse_categorical_crossentropy'  \n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Embedding(inp, nodes))\n",
        "    if method == 'LSTM':\n",
        "      if regularizer == None:\n",
        "        model.add(layers.LSTM(nodes, activation=activation, return_sequences=stacking))\n",
        "      if regularizer == 'L1':\n",
        "        model.add(layers.LSTM(nodes, activation=activation, kernel_regularizer=l1(0.005), return_sequences=stacking))\n",
        "      if regularizer == 'L2':\n",
        "        model.add(layers.LSTM(nodes, activation=activation, kernel_regularizer=L2(0.005), return_sequences=stacking))\n",
        "    if method == 'GRU':\n",
        "      if regularizer == None:\n",
        "        model.add(layers.GRU(nodes, activation=activation, return_sequences=stacking))\n",
        "      if regularizer == 'L1':\n",
        "        model.add(layers.GRU(nodes, activation=activation, kernel_regularizer=l1(0.005), return_sequences=stacking))\n",
        "      if regularizer == 'L2':\n",
        "        model.add(layers.GRU(nodes, activation=activation, kernel_regularizer=L2(0.005), return_sequences=stacking))\n",
        "    if dropout == True:\n",
        "      model.add(layers.Dropout(0.5))\n",
        "    dl = DeepLearning()\n",
        "    if stacking == True:\n",
        "      nodes = nodes//2\n",
        "      if method == 'LSTM':\n",
        "        model.add(layers.LSTM(nodes, activation=activation))\n",
        "      else:\n",
        "        model.add(layers.GRU(nodes, activation=activation))\n",
        "    p = dl.Powers(nodes)\n",
        "    for i in range(p):\n",
        "      nodes /= 2\n",
        "      nodes = int(nodes)\n",
        "      if nodes > 16:\n",
        "        if nodes > nclasses:\n",
        "          model.add(layers.Dense(nodes, activation=activation))\n",
        "    model.add(layers.Dense(pen, activation=activation))\n",
        "    model.add(layers.Dense(nclasses, activation=oa))\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  def TestDL(self, params, func, task, X, y, X_val=None, y_val=None, batch_size=64, epochs=50):\n",
        "    early_stopping = [EarlyStopping(patience=10), ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5')]\n",
        "    if task == 'classification':\n",
        "      k = KerasClassifier(func)\n",
        "    if task == 'regression':\n",
        "      k = KerasRegressor(func)\n",
        "    grid = GridSearchCV(k, params, cv=3)\n",
        "    if type(X_val) != np.ndarray:\n",
        "      grid.fit(X, y, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "    else:\n",
        "      grid.fit(X, y, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), callbacks=early_stopping)\n",
        "    return grid.best_params_"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-YJyFvW1bbX"
      },
      "source": [
        "dh = DataHelper()\n",
        "dl = DeepLearning()\n",
        "X, y = dh.ModelReadyText1(df, 'tweet', 'is_there_an_emotion_directed_at_a_brand_or_product', pad=33)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5sxxQwpAJic",
        "outputId": "cc4736bd-7a22-4f0c-8342-1946cb8abb2c"
      },
      "source": [
        "def Bidirectional(vocab, nodes, output, optimizer, metrics, activation):\r\n",
        "  if output == 1:\r\n",
        "    oa = 'linear'\r\n",
        "    loss = 'mse'\r\n",
        "  if output == 2:\r\n",
        "    oa = 'sigmoid'\r\n",
        "    loss = 'binary_crossentropy'\r\n",
        "  else:\r\n",
        "    oa = 'softmax'\r\n",
        "    loss = 'categorical_crossentropy'\r\n",
        "  model = models.Sequential()\r\n",
        "  model.add(layers.Embedding(vocab, nodes)) \r\n",
        "  model.add(layers.Bidirectional(layers.LSTM(nodes))) \r\n",
        "  model.add(layers.Dense(nodes/2, activation=activation))\r\n",
        "  model.add(layers.Dense(nodes/4, activation=activation))\r\n",
        "  model.add(layers.Dense(output, activation=oa))\r\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\r\n",
        "  return model\r\n",
        "\r\n",
        "B = Bidirectional(21444, 64, 3, 'adam', ['accuracy'], 'relu')\r\n",
        "early_stopping = [EarlyStopping(patience=10), ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5')]\r\n",
        "B.summary()\r\n",
        "B.fit(X_train, y_train, batch_size=64, epochs=50, callbacks=early_stopping, validation_split=0.2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          1372416   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               66048     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 1,443,171\n",
            "Trainable params: 1,443,171\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "101/101 [==============================] - 3s 28ms/step - loss: 0.8705 - accuracy: 0.5989 - val_loss: 0.7880 - val_accuracy: 0.6153\n",
            "Epoch 2/50\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.6993 - accuracy: 0.6852 - val_loss: 0.7559 - val_accuracy: 0.6743\n",
            "Epoch 3/50\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.5196 - accuracy: 0.7900 - val_loss: 0.8024 - val_accuracy: 0.6812\n",
            "Epoch 4/50\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.3714 - accuracy: 0.8581 - val_loss: 0.8683 - val_accuracy: 0.6687\n",
            "Epoch 5/50\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.2789 - accuracy: 0.8932 - val_loss: 0.9340 - val_accuracy: 0.6948\n",
            "Epoch 6/50\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.2299 - accuracy: 0.9105 - val_loss: 1.0578 - val_accuracy: 0.6656\n",
            "Epoch 7/50\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.2054 - accuracy: 0.9167 - val_loss: 1.0377 - val_accuracy: 0.6768\n",
            "Epoch 8/50\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.1862 - accuracy: 0.9237 - val_loss: 1.0291 - val_accuracy: 0.6880\n",
            "Epoch 9/50\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.1685 - accuracy: 0.9269 - val_loss: 1.1891 - val_accuracy: 0.6861\n",
            "Epoch 10/50\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.1594 - accuracy: 0.9327 - val_loss: 1.2878 - val_accuracy: 0.6756\n",
            "Epoch 11/50\n",
            "101/101 [==============================] - 2s 21ms/step - loss: 0.1446 - accuracy: 0.9341 - val_loss: 1.2422 - val_accuracy: 0.6743\n",
            "Epoch 12/50\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.1352 - accuracy: 0.9363 - val_loss: 1.3095 - val_accuracy: 0.6793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f41dac65dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOBN48GwDYBC",
        "outputId": "6b61662e-dcaa-4523-d5fb-a9c13b387e3d"
      },
      "source": [
        "params = {'nodes': [50, 64, 128, 256], 'activation': ['relu', 'tanh'], 'optimizer': ['rmsprop', 'adam'], 'metrics': ['accuracy'], 'vocab': [21444], 'output': [3]}\r\n",
        "\r\n",
        "dl.TestDL(params, Bidirectional, 'classification', X_train, y_train, X_val=X_val, y_val=y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.8734 - accuracy: 0.5887 - val_loss: 0.8077 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.7723 - accuracy: 0.6388 - val_loss: 0.7872 - val_accuracy: 0.6387\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.6524 - accuracy: 0.7294 - val_loss: 0.7693 - val_accuracy: 0.6588\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.5526 - accuracy: 0.7784 - val_loss: 0.8426 - val_accuracy: 0.6661\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 14ms/step - loss: 0.4664 - accuracy: 0.8125 - val_loss: 0.8605 - val_accuracy: 0.6538\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.3960 - accuracy: 0.8511 - val_loss: 0.8981 - val_accuracy: 0.6572\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 14ms/step - loss: 0.3317 - accuracy: 0.8763 - val_loss: 0.8113 - val_accuracy: 0.6628\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 14ms/step - loss: 0.2858 - accuracy: 0.8956 - val_loss: 0.9946 - val_accuracy: 0.6477\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 14ms/step - loss: 0.2492 - accuracy: 0.9037 - val_loss: 1.1798 - val_accuracy: 0.6387\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.2245 - accuracy: 0.9175 - val_loss: 1.0654 - val_accuracy: 0.6549\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.1945 - accuracy: 0.9287 - val_loss: 1.1443 - val_accuracy: 0.6549\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.1802 - accuracy: 0.9312 - val_loss: 1.2593 - val_accuracy: 0.6510\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.1703 - accuracy: 0.9379 - val_loss: 1.2609 - val_accuracy: 0.6560\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.3337 - accuracy: 0.6357\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.8850 - accuracy: 0.5918 - val_loss: 0.8149 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 14ms/step - loss: 0.7877 - accuracy: 0.6293 - val_loss: 0.7716 - val_accuracy: 0.6309\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.6911 - accuracy: 0.7076 - val_loss: 0.7696 - val_accuracy: 0.6359\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.6220 - accuracy: 0.7412 - val_loss: 0.8520 - val_accuracy: 0.6426\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.5389 - accuracy: 0.7935 - val_loss: 0.8462 - val_accuracy: 0.6465\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.4910 - accuracy: 0.8176 - val_loss: 0.9298 - val_accuracy: 0.5464\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.4332 - accuracy: 0.8456 - val_loss: 0.9788 - val_accuracy: 0.6376\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.3844 - accuracy: 0.8573 - val_loss: 0.9775 - val_accuracy: 0.5587\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.3509 - accuracy: 0.8685 - val_loss: 1.0751 - val_accuracy: 0.6460\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 14ms/step - loss: 0.3114 - accuracy: 0.8884 - val_loss: 0.9744 - val_accuracy: 0.6588\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.2840 - accuracy: 0.8979 - val_loss: 0.9983 - val_accuracy: 0.6432\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.2666 - accuracy: 0.8973 - val_loss: 1.0980 - val_accuracy: 0.6460\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 1s 14ms/step - loss: 0.2337 - accuracy: 0.9124 - val_loss: 1.0558 - val_accuracy: 0.6292\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.1305 - accuracy: 0.6139\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.8656 - accuracy: 0.6004 - val_loss: 0.8066 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.7601 - accuracy: 0.6511 - val_loss: 0.7935 - val_accuracy: 0.6337\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.6596 - accuracy: 0.7146 - val_loss: 0.7706 - val_accuracy: 0.6258\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.5650 - accuracy: 0.7661 - val_loss: 0.7734 - val_accuracy: 0.6337\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 14ms/step - loss: 0.4841 - accuracy: 0.8106 - val_loss: 0.8506 - val_accuracy: 0.6230\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.4187 - accuracy: 0.8349 - val_loss: 0.9030 - val_accuracy: 0.6426\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.3622 - accuracy: 0.8660 - val_loss: 0.9775 - val_accuracy: 0.6477\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.3283 - accuracy: 0.8721 - val_loss: 0.9755 - val_accuracy: 0.6437\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.2873 - accuracy: 0.8928 - val_loss: 1.1088 - val_accuracy: 0.6320\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.2549 - accuracy: 0.9060 - val_loss: 0.9711 - val_accuracy: 0.6376\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.2394 - accuracy: 0.9130 - val_loss: 1.1070 - val_accuracy: 0.6421\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 14ms/step - loss: 0.2188 - accuracy: 0.9175 - val_loss: 1.1274 - val_accuracy: 0.6370\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.2081 - accuracy: 0.9211 - val_loss: 1.1279 - val_accuracy: 0.6432\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.1604 - accuracy: 0.6447\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.9102 - accuracy: 0.5884 - val_loss: 0.8248 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.7825 - accuracy: 0.6304 - val_loss: 0.7747 - val_accuracy: 0.6527\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.6134 - accuracy: 0.7633 - val_loss: 0.7992 - val_accuracy: 0.6583\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.4785 - accuracy: 0.8318 - val_loss: 0.8918 - val_accuracy: 0.6353\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.4137 - accuracy: 0.8579 - val_loss: 0.8594 - val_accuracy: 0.6516\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.3871 - accuracy: 0.8640 - val_loss: 0.9291 - val_accuracy: 0.6404\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.3486 - accuracy: 0.8794 - val_loss: 1.1104 - val_accuracy: 0.6583\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.3156 - accuracy: 0.8892 - val_loss: 1.0987 - val_accuracy: 0.6107\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2736 - accuracy: 0.8984 - val_loss: 1.2872 - val_accuracy: 0.6421\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.2436 - accuracy: 0.9001 - val_loss: 1.3917 - val_accuracy: 0.6443\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2196 - accuracy: 0.9127 - val_loss: 1.7027 - val_accuracy: 0.6460\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1931 - accuracy: 0.9264 - val_loss: 1.4322 - val_accuracy: 0.6477\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.5393 - accuracy: 0.6279\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.9192 - accuracy: 0.5940 - val_loss: 0.8257 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.7785 - accuracy: 0.6298 - val_loss: 0.7705 - val_accuracy: 0.6258\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.6233 - accuracy: 0.7373 - val_loss: 0.7790 - val_accuracy: 0.6449\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.4753 - accuracy: 0.8041 - val_loss: 0.8241 - val_accuracy: 0.6504\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.3546 - accuracy: 0.8685 - val_loss: 0.9287 - val_accuracy: 0.6555\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2826 - accuracy: 0.8965 - val_loss: 0.9489 - val_accuracy: 0.6426\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2110 - accuracy: 0.9203 - val_loss: 1.0972 - val_accuracy: 0.6538\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.1796 - accuracy: 0.9281 - val_loss: 1.1953 - val_accuracy: 0.6096\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.1567 - accuracy: 0.9410 - val_loss: 1.3502 - val_accuracy: 0.6510\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.1480 - accuracy: 0.9424 - val_loss: 1.1952 - val_accuracy: 0.6426\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1314 - accuracy: 0.9468 - val_loss: 1.4102 - val_accuracy: 0.6477\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.1206 - accuracy: 0.9510 - val_loss: 1.5089 - val_accuracy: 0.6309\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.6084 - accuracy: 0.6223\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.8984 - accuracy: 0.6041 - val_loss: 0.8174 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.7826 - accuracy: 0.6245 - val_loss: 0.7732 - val_accuracy: 0.6337\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.6317 - accuracy: 0.7247 - val_loss: 0.7751 - val_accuracy: 0.6387\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.4859 - accuracy: 0.7994 - val_loss: 0.8292 - val_accuracy: 0.6225\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.3813 - accuracy: 0.8537 - val_loss: 0.9008 - val_accuracy: 0.6398\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2888 - accuracy: 0.8976 - val_loss: 1.0305 - val_accuracy: 0.6370\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2337 - accuracy: 0.9180 - val_loss: 1.0507 - val_accuracy: 0.6381\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1890 - accuracy: 0.9281 - val_loss: 1.2374 - val_accuracy: 0.6353\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1716 - accuracy: 0.9407 - val_loss: 1.1971 - val_accuracy: 0.6292\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1568 - accuracy: 0.9418 - val_loss: 1.2865 - val_accuracy: 0.6454\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1475 - accuracy: 0.9440 - val_loss: 1.3387 - val_accuracy: 0.6393\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1372 - accuracy: 0.9463 - val_loss: 1.3616 - val_accuracy: 0.6337\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.3619 - accuracy: 0.6368\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 32ms/step - loss: 0.8632 - accuracy: 0.5887 - val_loss: 0.8105 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.7815 - accuracy: 0.6198 - val_loss: 0.7793 - val_accuracy: 0.6381\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.6605 - accuracy: 0.7065 - val_loss: 0.8054 - val_accuracy: 0.6119\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.5368 - accuracy: 0.7843 - val_loss: 0.8468 - val_accuracy: 0.6516\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.4466 - accuracy: 0.8279 - val_loss: 0.8386 - val_accuracy: 0.6605\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.3700 - accuracy: 0.8626 - val_loss: 1.0396 - val_accuracy: 0.6398\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.3209 - accuracy: 0.8828 - val_loss: 1.0124 - val_accuracy: 0.6353\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.2796 - accuracy: 0.8956 - val_loss: 1.0047 - val_accuracy: 0.6454\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.2439 - accuracy: 0.9079 - val_loss: 1.3213 - val_accuracy: 0.6482\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.2212 - accuracy: 0.9186 - val_loss: 1.2889 - val_accuracy: 0.5973\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.2007 - accuracy: 0.9278 - val_loss: 1.3562 - val_accuracy: 0.6577\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.1882 - accuracy: 0.9289 - val_loss: 1.2801 - val_accuracy: 0.6326\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.4046 - accuracy: 0.6212\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 1s 27ms/step - loss: 0.8742 - accuracy: 0.5736 - val_loss: 0.8340 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.7922 - accuracy: 0.6052 - val_loss: 0.7812 - val_accuracy: 0.6225\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.6992 - accuracy: 0.6802 - val_loss: 0.7593 - val_accuracy: 0.6482\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.5872 - accuracy: 0.7532 - val_loss: 0.7560 - val_accuracy: 0.6560\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.4835 - accuracy: 0.8142 - val_loss: 0.9665 - val_accuracy: 0.6421\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.4094 - accuracy: 0.8439 - val_loss: 0.8740 - val_accuracy: 0.6661\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.3504 - accuracy: 0.8702 - val_loss: 0.8947 - val_accuracy: 0.6454\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.3043 - accuracy: 0.8839 - val_loss: 0.9854 - val_accuracy: 0.6560\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.2692 - accuracy: 0.9010 - val_loss: 0.9454 - val_accuracy: 0.6678\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.2428 - accuracy: 0.9099 - val_loss: 1.1181 - val_accuracy: 0.6376\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.2228 - accuracy: 0.9161 - val_loss: 1.3602 - val_accuracy: 0.6700\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.1965 - accuracy: 0.9228 - val_loss: 1.1593 - val_accuracy: 0.6314\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.1883 - accuracy: 0.9233 - val_loss: 1.3315 - val_accuracy: 0.6644\n",
            "Epoch 14/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.1759 - accuracy: 0.9312 - val_loss: 1.2919 - val_accuracy: 0.5789\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.3604 - accuracy: 0.5590\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.8594 - accuracy: 0.6007 - val_loss: 0.8095 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.7619 - accuracy: 0.6396 - val_loss: 0.8072 - val_accuracy: 0.6264\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.6648 - accuracy: 0.7140 - val_loss: 0.7957 - val_accuracy: 0.5996\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.5834 - accuracy: 0.7608 - val_loss: 0.8441 - val_accuracy: 0.6353\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.5206 - accuracy: 0.7952 - val_loss: 0.9206 - val_accuracy: 0.5934\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.4486 - accuracy: 0.8276 - val_loss: 0.8544 - val_accuracy: 0.6124\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.3903 - accuracy: 0.8553 - val_loss: 0.8570 - val_accuracy: 0.6437\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.3352 - accuracy: 0.8755 - val_loss: 0.8946 - val_accuracy: 0.6247\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.2980 - accuracy: 0.8892 - val_loss: 1.0805 - val_accuracy: 0.6454\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.2646 - accuracy: 0.9046 - val_loss: 1.1063 - val_accuracy: 0.6398\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.2373 - accuracy: 0.9068 - val_loss: 1.0649 - val_accuracy: 0.6437\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.2151 - accuracy: 0.9177 - val_loss: 1.1965 - val_accuracy: 0.6477\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.2038 - accuracy: 0.9219 - val_loss: 1.3949 - val_accuracy: 0.6191\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.4737 - accuracy: 0.6184\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 32ms/step - loss: 0.8879 - accuracy: 0.5971 - val_loss: 0.8269 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.8056 - accuracy: 0.5971 - val_loss: 0.7936 - val_accuracy: 0.6214\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.6610 - accuracy: 0.7071 - val_loss: 0.8383 - val_accuracy: 0.6504\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.4639 - accuracy: 0.8218 - val_loss: 0.8361 - val_accuracy: 0.6326\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.3283 - accuracy: 0.8833 - val_loss: 0.9923 - val_accuracy: 0.6415\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.2405 - accuracy: 0.9191 - val_loss: 0.9558 - val_accuracy: 0.6230\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2085 - accuracy: 0.9281 - val_loss: 1.1151 - val_accuracy: 0.6460\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1726 - accuracy: 0.9412 - val_loss: 1.2924 - val_accuracy: 0.6471\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1483 - accuracy: 0.9494 - val_loss: 1.2796 - val_accuracy: 0.6449\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1361 - accuracy: 0.9502 - val_loss: 1.3155 - val_accuracy: 0.6421\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1266 - accuracy: 0.9550 - val_loss: 1.3129 - val_accuracy: 0.6415\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1123 - accuracy: 0.9575 - val_loss: 1.4370 - val_accuracy: 0.6326\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.5209 - accuracy: 0.6295\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 32ms/step - loss: 0.8923 - accuracy: 0.5985 - val_loss: 0.8078 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.7511 - accuracy: 0.6463 - val_loss: 0.7481 - val_accuracy: 0.6499\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.5630 - accuracy: 0.7569 - val_loss: 0.7623 - val_accuracy: 0.6611\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.3876 - accuracy: 0.8495 - val_loss: 0.8498 - val_accuracy: 0.6583\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.2641 - accuracy: 0.9015 - val_loss: 0.9596 - val_accuracy: 0.6544\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.2084 - accuracy: 0.9225 - val_loss: 1.0087 - val_accuracy: 0.6577\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1623 - accuracy: 0.9384 - val_loss: 1.0567 - val_accuracy: 0.6516\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1452 - accuracy: 0.9443 - val_loss: 1.1352 - val_accuracy: 0.6516\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1250 - accuracy: 0.9474 - val_loss: 1.2928 - val_accuracy: 0.6499\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1234 - accuracy: 0.9505 - val_loss: 1.2497 - val_accuracy: 0.6432\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1131 - accuracy: 0.9527 - val_loss: 1.3423 - val_accuracy: 0.6622\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1056 - accuracy: 0.9566 - val_loss: 1.3222 - val_accuracy: 0.6594\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.4389 - accuracy: 0.6402\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 32ms/step - loss: 0.8902 - accuracy: 0.5478 - val_loss: 0.8174 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.7887 - accuracy: 0.6200 - val_loss: 0.7650 - val_accuracy: 0.6376\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.6091 - accuracy: 0.7364 - val_loss: 0.7463 - val_accuracy: 0.6465\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.3980 - accuracy: 0.8464 - val_loss: 0.8190 - val_accuracy: 0.6477\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.2735 - accuracy: 0.9007 - val_loss: 0.8978 - val_accuracy: 0.6482\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.2071 - accuracy: 0.9228 - val_loss: 1.1073 - val_accuracy: 0.6477\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1827 - accuracy: 0.9303 - val_loss: 1.0584 - val_accuracy: 0.6415\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1547 - accuracy: 0.9429 - val_loss: 1.2310 - val_accuracy: 0.6465\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1474 - accuracy: 0.9412 - val_loss: 1.3261 - val_accuracy: 0.6387\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1269 - accuracy: 0.9460 - val_loss: 1.3231 - val_accuracy: 0.6488\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.1173 - accuracy: 0.9491 - val_loss: 1.3800 - val_accuracy: 0.6348\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1125 - accuracy: 0.9522 - val_loss: 1.4860 - val_accuracy: 0.6314\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0968 - accuracy: 0.9550 - val_loss: 1.5035 - val_accuracy: 0.6393\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.5126 - accuracy: 0.6430\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 0.8607 - accuracy: 0.5865 - val_loss: 0.7979 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.7531 - accuracy: 0.6466 - val_loss: 0.7668 - val_accuracy: 0.6499\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.5929 - accuracy: 0.7518 - val_loss: 0.7540 - val_accuracy: 0.6700\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.4647 - accuracy: 0.8201 - val_loss: 0.8154 - val_accuracy: 0.6633\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.3764 - accuracy: 0.8576 - val_loss: 1.0303 - val_accuracy: 0.5951\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.3172 - accuracy: 0.8730 - val_loss: 0.9486 - val_accuracy: 0.6504\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 27ms/step - loss: 0.2581 - accuracy: 0.9032 - val_loss: 1.0427 - val_accuracy: 0.6555\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.2192 - accuracy: 0.9183 - val_loss: 1.1220 - val_accuracy: 0.6471\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1953 - accuracy: 0.9259 - val_loss: 1.2311 - val_accuracy: 0.6538\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1769 - accuracy: 0.9317 - val_loss: 1.2559 - val_accuracy: 0.6342\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1548 - accuracy: 0.9401 - val_loss: 1.6125 - val_accuracy: 0.6393\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1432 - accuracy: 0.9460 - val_loss: 1.4680 - val_accuracy: 0.6421\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1280 - accuracy: 0.9513 - val_loss: 1.6587 - val_accuracy: 0.6303\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.7714 - accuracy: 0.6200\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 34ms/step - loss: 0.8814 - accuracy: 0.5949 - val_loss: 0.7995 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.7583 - accuracy: 0.6469 - val_loss: 0.7480 - val_accuracy: 0.6443\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.6176 - accuracy: 0.7339 - val_loss: 0.7589 - val_accuracy: 0.6460\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.4717 - accuracy: 0.8151 - val_loss: 0.8455 - val_accuracy: 0.6650\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.3925 - accuracy: 0.8503 - val_loss: 0.7831 - val_accuracy: 0.6678\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.3297 - accuracy: 0.8749 - val_loss: 0.9956 - val_accuracy: 0.6622\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.2890 - accuracy: 0.8914 - val_loss: 1.0620 - val_accuracy: 0.6454\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.2518 - accuracy: 0.9001 - val_loss: 1.0116 - val_accuracy: 0.6404\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.2458 - accuracy: 0.9074 - val_loss: 1.2578 - val_accuracy: 0.6588\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1974 - accuracy: 0.9214 - val_loss: 1.1129 - val_accuracy: 0.6577\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1846 - accuracy: 0.9273 - val_loss: 1.2004 - val_accuracy: 0.6600\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1753 - accuracy: 0.9256 - val_loss: 1.6019 - val_accuracy: 0.6516\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.6592 - accuracy: 0.6491\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 33ms/step - loss: 0.8551 - accuracy: 0.6116 - val_loss: 0.8300 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.7442 - accuracy: 0.6542 - val_loss: 0.7565 - val_accuracy: 0.6538\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.6246 - accuracy: 0.7350 - val_loss: 0.8116 - val_accuracy: 0.6622\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.5223 - accuracy: 0.7756 - val_loss: 0.8332 - val_accuracy: 0.6454\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.4307 - accuracy: 0.8386 - val_loss: 0.8508 - val_accuracy: 0.6577\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.3531 - accuracy: 0.8677 - val_loss: 0.8782 - val_accuracy: 0.6616\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.2833 - accuracy: 0.8982 - val_loss: 1.2147 - val_accuracy: 0.6566\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.2546 - accuracy: 0.9012 - val_loss: 1.0153 - val_accuracy: 0.6594\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.2238 - accuracy: 0.9127 - val_loss: 1.1420 - val_accuracy: 0.6482\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1932 - accuracy: 0.9275 - val_loss: 1.0940 - val_accuracy: 0.6538\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1708 - accuracy: 0.9323 - val_loss: 1.3026 - val_accuracy: 0.6387\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1621 - accuracy: 0.9354 - val_loss: 1.4716 - val_accuracy: 0.6544\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.4780 - accuracy: 0.6637\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 3s 46ms/step - loss: 0.8617 - accuracy: 0.5915 - val_loss: 0.7958 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 0.7376 - accuracy: 0.6458 - val_loss: 0.7991 - val_accuracy: 0.6398\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 0.5389 - accuracy: 0.7770 - val_loss: 0.8129 - val_accuracy: 0.6443\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 0.3714 - accuracy: 0.8632 - val_loss: 1.0112 - val_accuracy: 0.6208\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.2573 - accuracy: 0.9093 - val_loss: 1.0530 - val_accuracy: 0.6426\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 0.1747 - accuracy: 0.9396 - val_loss: 1.3309 - val_accuracy: 0.6432\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 0.1544 - accuracy: 0.9474 - val_loss: 1.2925 - val_accuracy: 0.6381\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 0.1298 - accuracy: 0.9513 - val_loss: 1.1984 - val_accuracy: 0.6409\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.1167 - accuracy: 0.9555 - val_loss: 1.3690 - val_accuracy: 0.6309\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 0.1132 - accuracy: 0.9594 - val_loss: 1.3403 - val_accuracy: 0.6398\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 0.0930 - accuracy: 0.9631 - val_loss: 1.3305 - val_accuracy: 0.6443\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.3944 - accuracy: 0.6419\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 3s 58ms/step - loss: 0.8813 - accuracy: 0.5976 - val_loss: 0.8122 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.7520 - accuracy: 0.6421 - val_loss: 0.7814 - val_accuracy: 0.6342\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.5529 - accuracy: 0.7664 - val_loss: 0.7702 - val_accuracy: 0.6611\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.3688 - accuracy: 0.8553 - val_loss: 0.8987 - val_accuracy: 0.6510\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.2442 - accuracy: 0.9074 - val_loss: 0.9577 - val_accuracy: 0.6572\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.1896 - accuracy: 0.9275 - val_loss: 1.0285 - val_accuracy: 0.6504\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.1537 - accuracy: 0.9373 - val_loss: 1.1827 - val_accuracy: 0.6398\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1414 - accuracy: 0.9468 - val_loss: 1.2507 - val_accuracy: 0.6538\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.1236 - accuracy: 0.9466 - val_loss: 1.4910 - val_accuracy: 0.6381\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.1282 - accuracy: 0.9485 - val_loss: 1.3369 - val_accuracy: 0.6353\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.0985 - accuracy: 0.9578 - val_loss: 1.4196 - val_accuracy: 0.6471\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.0970 - accuracy: 0.9552 - val_loss: 1.4742 - val_accuracy: 0.6342\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.0948 - accuracy: 0.9547 - val_loss: 1.4713 - val_accuracy: 0.6409\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.5377 - accuracy: 0.6385\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 3s 47ms/step - loss: 0.8651 - accuracy: 0.6027 - val_loss: 0.8054 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.7458 - accuracy: 0.6332 - val_loss: 0.7454 - val_accuracy: 0.6510\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.5107 - accuracy: 0.7876 - val_loss: 0.7703 - val_accuracy: 0.6538\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 0.3089 - accuracy: 0.8811 - val_loss: 0.8982 - val_accuracy: 0.6504\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.2314 - accuracy: 0.9079 - val_loss: 1.0398 - val_accuracy: 0.6460\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.1835 - accuracy: 0.9292 - val_loss: 1.1415 - val_accuracy: 0.6437\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.1547 - accuracy: 0.9412 - val_loss: 1.1162 - val_accuracy: 0.6409\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.1300 - accuracy: 0.9480 - val_loss: 1.2417 - val_accuracy: 0.6482\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.1194 - accuracy: 0.9505 - val_loss: 1.2376 - val_accuracy: 0.6286\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.1077 - accuracy: 0.9524 - val_loss: 1.5182 - val_accuracy: 0.6236\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 0.1013 - accuracy: 0.9550 - val_loss: 1.3450 - val_accuracy: 0.6342\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 0.0894 - accuracy: 0.9594 - val_loss: 1.7161 - val_accuracy: 0.6421\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.6887 - accuracy: 0.6542\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 3s 53ms/step - loss: 0.8612 - accuracy: 0.5904 - val_loss: 0.7907 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.7553 - accuracy: 0.6371 - val_loss: 0.7750 - val_accuracy: 0.6337\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.5941 - accuracy: 0.7518 - val_loss: 0.7808 - val_accuracy: 0.6504\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.4517 - accuracy: 0.8195 - val_loss: 0.9295 - val_accuracy: 0.6135\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.3552 - accuracy: 0.8595 - val_loss: 0.8784 - val_accuracy: 0.6437\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.2771 - accuracy: 0.8931 - val_loss: 1.2065 - val_accuracy: 0.6096\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.2457 - accuracy: 0.9015 - val_loss: 1.1630 - val_accuracy: 0.6639\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.2004 - accuracy: 0.9239 - val_loss: 1.0663 - val_accuracy: 0.6572\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1763 - accuracy: 0.9301 - val_loss: 1.1672 - val_accuracy: 0.6381\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 2s 41ms/step - loss: 0.1503 - accuracy: 0.9415 - val_loss: 1.6251 - val_accuracy: 0.6583\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.1365 - accuracy: 0.9494 - val_loss: 1.6398 - val_accuracy: 0.6449\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.1249 - accuracy: 0.9524 - val_loss: 1.5256 - val_accuracy: 0.6527\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.6563 - accuracy: 0.6469\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 3s 52ms/step - loss: 0.8527 - accuracy: 0.5954 - val_loss: 0.7830 - val_accuracy: 0.6180\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.7399 - accuracy: 0.6547 - val_loss: 0.7595 - val_accuracy: 0.6275\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.5743 - accuracy: 0.7588 - val_loss: 0.8998 - val_accuracy: 0.6376\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.4433 - accuracy: 0.8251 - val_loss: 0.8556 - val_accuracy: 0.6538\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.3567 - accuracy: 0.8565 - val_loss: 0.8630 - val_accuracy: 0.6348\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.2844 - accuracy: 0.8889 - val_loss: 1.1198 - val_accuracy: 0.5755\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.2370 - accuracy: 0.9093 - val_loss: 1.1266 - val_accuracy: 0.6370\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.2058 - accuracy: 0.9186 - val_loss: 1.0239 - val_accuracy: 0.6135\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.1724 - accuracy: 0.9306 - val_loss: 1.1711 - val_accuracy: 0.6091\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1688 - accuracy: 0.9298 - val_loss: 1.4034 - val_accuracy: 0.6588\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.1448 - accuracy: 0.9331 - val_loss: 1.6412 - val_accuracy: 0.6348\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1326 - accuracy: 0.9412 - val_loss: 1.6050 - val_accuracy: 0.6225\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5617 - accuracy: 0.6374\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 3s 51ms/step - loss: 0.8615 - accuracy: 0.6094 - val_loss: 0.9000 - val_accuracy: 0.5006\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.7151 - accuracy: 0.6858 - val_loss: 0.8585 - val_accuracy: 0.6359\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.5824 - accuracy: 0.7546 - val_loss: 0.8151 - val_accuracy: 0.6706\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.4364 - accuracy: 0.8254 - val_loss: 0.8650 - val_accuracy: 0.6689\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.3475 - accuracy: 0.8710 - val_loss: 0.8105 - val_accuracy: 0.6717\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.2743 - accuracy: 0.8931 - val_loss: 0.9443 - val_accuracy: 0.6538\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.2317 - accuracy: 0.9116 - val_loss: 1.0972 - val_accuracy: 0.6549\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.2072 - accuracy: 0.9211 - val_loss: 1.0831 - val_accuracy: 0.6644\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1767 - accuracy: 0.9309 - val_loss: 1.1018 - val_accuracy: 0.6538\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1539 - accuracy: 0.9384 - val_loss: 1.1462 - val_accuracy: 0.6298\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.1343 - accuracy: 0.9477 - val_loss: 1.5399 - val_accuracy: 0.6583\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.1234 - accuracy: 0.9463 - val_loss: 1.4217 - val_accuracy: 0.6639\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 2s 41ms/step - loss: 0.1136 - accuracy: 0.9477 - val_loss: 1.8173 - val_accuracy: 0.6381\n",
            "Epoch 14/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.1043 - accuracy: 0.9502 - val_loss: 1.9363 - val_accuracy: 0.6393\n",
            "Epoch 15/50\n",
            "56/56 [==============================] - 2s 41ms/step - loss: 0.0988 - accuracy: 0.9558 - val_loss: 1.6422 - val_accuracy: 0.6281\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.6952 - accuracy: 0.6184\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 5s 81ms/step - loss: 0.8579 - accuracy: 0.5971 - val_loss: 0.7842 - val_accuracy: 0.6281\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.6610 - accuracy: 0.7073 - val_loss: 0.7641 - val_accuracy: 0.6477\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 4s 71ms/step - loss: 0.4093 - accuracy: 0.8458 - val_loss: 0.8720 - val_accuracy: 0.6572\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 4s 72ms/step - loss: 0.2606 - accuracy: 0.9018 - val_loss: 0.9466 - val_accuracy: 0.6225\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 4s 70ms/step - loss: 0.1900 - accuracy: 0.9298 - val_loss: 1.1438 - val_accuracy: 0.6443\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 4s 71ms/step - loss: 0.1593 - accuracy: 0.9429 - val_loss: 1.1197 - val_accuracy: 0.6583\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 4s 71ms/step - loss: 0.1389 - accuracy: 0.9491 - val_loss: 1.1747 - val_accuracy: 0.6471\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 4s 72ms/step - loss: 0.1076 - accuracy: 0.9586 - val_loss: 1.3322 - val_accuracy: 0.6549\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.0975 - accuracy: 0.9647 - val_loss: 1.5211 - val_accuracy: 0.6348\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 4s 72ms/step - loss: 0.1314 - accuracy: 0.9524 - val_loss: 1.3825 - val_accuracy: 0.6437\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 4s 71ms/step - loss: 0.0956 - accuracy: 0.9636 - val_loss: 1.3364 - val_accuracy: 0.6225\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 4s 72ms/step - loss: 0.0890 - accuracy: 0.9636 - val_loss: 1.5054 - val_accuracy: 0.6202\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5913 - accuracy: 0.6178\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 5s 83ms/step - loss: 0.8631 - accuracy: 0.5971 - val_loss: 0.7928 - val_accuracy: 0.6314\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.6857 - accuracy: 0.6886 - val_loss: 0.7549 - val_accuracy: 0.6460\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.4388 - accuracy: 0.8257 - val_loss: 0.8795 - val_accuracy: 0.6538\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.2881 - accuracy: 0.8844 - val_loss: 0.8847 - val_accuracy: 0.6583\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 4s 70ms/step - loss: 0.2191 - accuracy: 0.9107 - val_loss: 1.1325 - val_accuracy: 0.6214\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 4s 72ms/step - loss: 0.1778 - accuracy: 0.9328 - val_loss: 1.2007 - val_accuracy: 0.6616\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 4s 71ms/step - loss: 0.1448 - accuracy: 0.9398 - val_loss: 1.2012 - val_accuracy: 0.6421\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 4s 72ms/step - loss: 0.1373 - accuracy: 0.9454 - val_loss: 1.2723 - val_accuracy: 0.6594\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 4s 69ms/step - loss: 0.1160 - accuracy: 0.9496 - val_loss: 1.4384 - val_accuracy: 0.6281\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 4s 70ms/step - loss: 0.1004 - accuracy: 0.9564 - val_loss: 1.5583 - val_accuracy: 0.6471\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 4s 69ms/step - loss: 0.0883 - accuracy: 0.9538 - val_loss: 1.9334 - val_accuracy: 0.6359\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 4s 70ms/step - loss: 0.0887 - accuracy: 0.9591 - val_loss: 1.9621 - val_accuracy: 0.6454\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 2.0996 - accuracy: 0.6363\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 5s 84ms/step - loss: 0.8559 - accuracy: 0.6002 - val_loss: 0.7911 - val_accuracy: 0.6169\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.7074 - accuracy: 0.6861 - val_loss: 0.7680 - val_accuracy: 0.6544\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 4s 68ms/step - loss: 0.4722 - accuracy: 0.8167 - val_loss: 0.8108 - val_accuracy: 0.6605\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 4s 71ms/step - loss: 0.3132 - accuracy: 0.8881 - val_loss: 0.9006 - val_accuracy: 0.6482\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 4s 71ms/step - loss: 0.2273 - accuracy: 0.9138 - val_loss: 1.1015 - val_accuracy: 0.6628\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 4s 72ms/step - loss: 0.1807 - accuracy: 0.9314 - val_loss: 1.0908 - val_accuracy: 0.6353\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 4s 70ms/step - loss: 0.1523 - accuracy: 0.9426 - val_loss: 1.2439 - val_accuracy: 0.6488\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 4s 71ms/step - loss: 0.1373 - accuracy: 0.9446 - val_loss: 1.1792 - val_accuracy: 0.6381\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 4s 70ms/step - loss: 0.1215 - accuracy: 0.9502 - val_loss: 1.6350 - val_accuracy: 0.6449\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 4s 71ms/step - loss: 0.1087 - accuracy: 0.9524 - val_loss: 1.3805 - val_accuracy: 0.6370\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 4s 69ms/step - loss: 0.0984 - accuracy: 0.9564 - val_loss: 1.5840 - val_accuracy: 0.6348\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 4s 70ms/step - loss: 0.0893 - accuracy: 0.9583 - val_loss: 1.5940 - val_accuracy: 0.6359\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5780 - accuracy: 0.6419\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 27ms/step - loss: 0.8465 - accuracy: 0.5979 - val_loss: 0.7946 - val_accuracy: 0.6242\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.7356 - accuracy: 0.6777 - val_loss: 0.7520 - val_accuracy: 0.6477\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.5908 - accuracy: 0.7563 - val_loss: 0.7395 - val_accuracy: 0.6728\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.4665 - accuracy: 0.8184 - val_loss: 0.8114 - val_accuracy: 0.6723\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.3802 - accuracy: 0.8539 - val_loss: 0.8234 - val_accuracy: 0.6700\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.3161 - accuracy: 0.8763 - val_loss: 1.0073 - val_accuracy: 0.6225\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.2766 - accuracy: 0.8926 - val_loss: 0.9231 - val_accuracy: 0.6689\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.2318 - accuracy: 0.9163 - val_loss: 0.9640 - val_accuracy: 0.6538\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.2082 - accuracy: 0.9231 - val_loss: 1.0896 - val_accuracy: 0.6577\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.1926 - accuracy: 0.9314 - val_loss: 1.1001 - val_accuracy: 0.6510\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.1675 - accuracy: 0.9390 - val_loss: 1.1607 - val_accuracy: 0.6527\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.1538 - accuracy: 0.9415 - val_loss: 1.2433 - val_accuracy: 0.6678\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.1455 - accuracy: 0.9452 - val_loss: 1.3638 - val_accuracy: 0.6068\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.5151 - accuracy: 0.5865\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.8547 - accuracy: 0.5990 - val_loss: 0.7886 - val_accuracy: 0.6186\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.7522 - accuracy: 0.6642 - val_loss: 0.7835 - val_accuracy: 0.6387\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.6382 - accuracy: 0.7269 - val_loss: 0.7825 - val_accuracy: 0.6460\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.5316 - accuracy: 0.7776 - val_loss: 0.8196 - val_accuracy: 0.6566\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.4319 - accuracy: 0.8327 - val_loss: 0.8840 - val_accuracy: 0.6588\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.3575 - accuracy: 0.8595 - val_loss: 0.9127 - val_accuracy: 0.6135\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.3117 - accuracy: 0.8811 - val_loss: 0.9063 - val_accuracy: 0.6409\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.2745 - accuracy: 0.8970 - val_loss: 1.0122 - val_accuracy: 0.6236\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.2372 - accuracy: 0.9107 - val_loss: 1.0171 - val_accuracy: 0.6398\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.2167 - accuracy: 0.9177 - val_loss: 1.0781 - val_accuracy: 0.6326\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.1955 - accuracy: 0.9247 - val_loss: 1.2002 - val_accuracy: 0.6102\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.1768 - accuracy: 0.9298 - val_loss: 1.1879 - val_accuracy: 0.6275\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.1624 - accuracy: 0.9292 - val_loss: 1.2504 - val_accuracy: 0.6398\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.3152 - accuracy: 0.6413\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.8373 - accuracy: 0.6063 - val_loss: 0.7912 - val_accuracy: 0.6186\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.7377 - accuracy: 0.6645 - val_loss: 0.7505 - val_accuracy: 0.6477\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.6215 - accuracy: 0.7387 - val_loss: 0.7437 - val_accuracy: 0.6594\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.4978 - accuracy: 0.7969 - val_loss: 0.8193 - val_accuracy: 0.6381\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.3968 - accuracy: 0.8444 - val_loss: 0.8364 - val_accuracy: 0.6650\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.3307 - accuracy: 0.8752 - val_loss: 0.8921 - val_accuracy: 0.6421\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.2862 - accuracy: 0.8931 - val_loss: 0.9752 - val_accuracy: 0.6365\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.2489 - accuracy: 0.9074 - val_loss: 0.9773 - val_accuracy: 0.6527\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.2138 - accuracy: 0.9180 - val_loss: 1.0004 - val_accuracy: 0.6516\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.1973 - accuracy: 0.9228 - val_loss: 1.0759 - val_accuracy: 0.6437\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.1729 - accuracy: 0.9287 - val_loss: 1.3264 - val_accuracy: 0.6219\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 0.1655 - accuracy: 0.9348 - val_loss: 1.1631 - val_accuracy: 0.6538\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.1487 - accuracy: 0.9421 - val_loss: 1.2334 - val_accuracy: 0.6409\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.2778 - accuracy: 0.6340\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.8758 - accuracy: 0.5960 - val_loss: 0.8221 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.7692 - accuracy: 0.6416 - val_loss: 0.7650 - val_accuracy: 0.6488\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.5734 - accuracy: 0.7692 - val_loss: 0.8098 - val_accuracy: 0.6409\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.4063 - accuracy: 0.8472 - val_loss: 0.9547 - val_accuracy: 0.6186\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2913 - accuracy: 0.9001 - val_loss: 0.9359 - val_accuracy: 0.6583\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1994 - accuracy: 0.9356 - val_loss: 0.9955 - val_accuracy: 0.6449\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1815 - accuracy: 0.9387 - val_loss: 1.0763 - val_accuracy: 0.6544\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1564 - accuracy: 0.9466 - val_loss: 1.1173 - val_accuracy: 0.6471\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1269 - accuracy: 0.9566 - val_loss: 1.1641 - val_accuracy: 0.6499\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1137 - accuracy: 0.9589 - val_loss: 1.1603 - val_accuracy: 0.6465\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1090 - accuracy: 0.9600 - val_loss: 1.1756 - val_accuracy: 0.6460\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1004 - accuracy: 0.9628 - val_loss: 1.2083 - val_accuracy: 0.6460\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.3183 - accuracy: 0.6223\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.8914 - accuracy: 0.5968 - val_loss: 0.8296 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.8004 - accuracy: 0.6323 - val_loss: 0.7672 - val_accuracy: 0.6365\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.6162 - accuracy: 0.7487 - val_loss: 0.7783 - val_accuracy: 0.6460\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.4608 - accuracy: 0.8237 - val_loss: 0.8342 - val_accuracy: 0.6326\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.3248 - accuracy: 0.8923 - val_loss: 0.8956 - val_accuracy: 0.6510\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2382 - accuracy: 0.9231 - val_loss: 0.9244 - val_accuracy: 0.6404\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.2007 - accuracy: 0.9326 - val_loss: 0.9810 - val_accuracy: 0.6538\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1764 - accuracy: 0.9396 - val_loss: 1.0437 - val_accuracy: 0.6449\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1510 - accuracy: 0.9488 - val_loss: 1.0455 - val_accuracy: 0.6622\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1353 - accuracy: 0.9482 - val_loss: 1.1178 - val_accuracy: 0.6555\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.1317 - accuracy: 0.9502 - val_loss: 1.1739 - val_accuracy: 0.6202\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1160 - accuracy: 0.9533 - val_loss: 1.2103 - val_accuracy: 0.6370\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.2847 - accuracy: 0.6256\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.8626 - accuracy: 0.5993 - val_loss: 0.8147 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.7453 - accuracy: 0.6584 - val_loss: 0.7425 - val_accuracy: 0.6482\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.5225 - accuracy: 0.7896 - val_loss: 0.7714 - val_accuracy: 0.6622\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.3351 - accuracy: 0.8777 - val_loss: 0.9035 - val_accuracy: 0.6616\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.2472 - accuracy: 0.9082 - val_loss: 0.9098 - val_accuracy: 0.6594\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.2003 - accuracy: 0.9264 - val_loss: 1.0725 - val_accuracy: 0.6460\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1644 - accuracy: 0.9398 - val_loss: 1.0580 - val_accuracy: 0.6605\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1514 - accuracy: 0.9398 - val_loss: 1.0908 - val_accuracy: 0.6409\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.1402 - accuracy: 0.9435 - val_loss: 1.0981 - val_accuracy: 0.6365\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1297 - accuracy: 0.9466 - val_loss: 1.2815 - val_accuracy: 0.6370\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1144 - accuracy: 0.9496 - val_loss: 1.2412 - val_accuracy: 0.6432\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.1012 - accuracy: 0.9544 - val_loss: 1.2882 - val_accuracy: 0.6449\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.3210 - accuracy: 0.6346\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.8434 - accuracy: 0.5890 - val_loss: 0.8229 - val_accuracy: 0.6219\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.7391 - accuracy: 0.6589 - val_loss: 0.7850 - val_accuracy: 0.6124\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.6104 - accuracy: 0.7457 - val_loss: 0.7888 - val_accuracy: 0.6499\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.4868 - accuracy: 0.8145 - val_loss: 0.8318 - val_accuracy: 0.6465\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.3825 - accuracy: 0.8514 - val_loss: 0.9537 - val_accuracy: 0.6074\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.3110 - accuracy: 0.8805 - val_loss: 0.9353 - val_accuracy: 0.6544\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.2544 - accuracy: 0.9024 - val_loss: 1.0434 - val_accuracy: 0.6482\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.2204 - accuracy: 0.9183 - val_loss: 1.2072 - val_accuracy: 0.6477\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.1918 - accuracy: 0.9284 - val_loss: 1.1591 - val_accuracy: 0.6471\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.1737 - accuracy: 0.9356 - val_loss: 1.3537 - val_accuracy: 0.6063\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.1601 - accuracy: 0.9387 - val_loss: 1.2913 - val_accuracy: 0.6174\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.1460 - accuracy: 0.9446 - val_loss: 1.2655 - val_accuracy: 0.6381\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.3635 - accuracy: 0.6200\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.8536 - accuracy: 0.6018 - val_loss: 0.7956 - val_accuracy: 0.6180\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.7523 - accuracy: 0.6491 - val_loss: 0.7664 - val_accuracy: 0.6387\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.6166 - accuracy: 0.7406 - val_loss: 0.7536 - val_accuracy: 0.6600\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.4862 - accuracy: 0.8061 - val_loss: 0.7756 - val_accuracy: 0.6560\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.4042 - accuracy: 0.8492 - val_loss: 0.8618 - val_accuracy: 0.6359\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.3355 - accuracy: 0.8719 - val_loss: 0.9457 - val_accuracy: 0.6107\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.2888 - accuracy: 0.8889 - val_loss: 0.9982 - val_accuracy: 0.6152\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.2536 - accuracy: 0.9049 - val_loss: 0.9853 - val_accuracy: 0.6521\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.2219 - accuracy: 0.9155 - val_loss: 1.0509 - val_accuracy: 0.6309\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.1987 - accuracy: 0.9225 - val_loss: 1.0677 - val_accuracy: 0.6544\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.1805 - accuracy: 0.9292 - val_loss: 1.1510 - val_accuracy: 0.6460\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.1675 - accuracy: 0.9334 - val_loss: 1.1318 - val_accuracy: 0.6516\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.1558 - accuracy: 0.9393 - val_loss: 1.2062 - val_accuracy: 0.6298\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.2920 - accuracy: 0.6133\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 1s 27ms/step - loss: 0.8450 - accuracy: 0.6027 - val_loss: 0.7955 - val_accuracy: 0.6208\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.7347 - accuracy: 0.6690 - val_loss: 0.7481 - val_accuracy: 0.6549\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.6030 - accuracy: 0.7406 - val_loss: 0.7763 - val_accuracy: 0.6409\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.4620 - accuracy: 0.8204 - val_loss: 0.8027 - val_accuracy: 0.6393\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.3749 - accuracy: 0.8525 - val_loss: 0.8293 - val_accuracy: 0.6521\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.3132 - accuracy: 0.8816 - val_loss: 0.9747 - val_accuracy: 0.6471\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.2715 - accuracy: 0.8951 - val_loss: 0.9997 - val_accuracy: 0.6521\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 0.2320 - accuracy: 0.9049 - val_loss: 1.0320 - val_accuracy: 0.6572\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.2058 - accuracy: 0.9214 - val_loss: 1.3054 - val_accuracy: 0.6147\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.1870 - accuracy: 0.9250 - val_loss: 1.1466 - val_accuracy: 0.6482\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.1665 - accuracy: 0.9309 - val_loss: 1.3394 - val_accuracy: 0.6404\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 0.1591 - accuracy: 0.9354 - val_loss: 1.2931 - val_accuracy: 0.6521\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.3158 - accuracy: 0.6458\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 34ms/step - loss: 0.8711 - accuracy: 0.5893 - val_loss: 0.8358 - val_accuracy: 0.5990\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.7120 - accuracy: 0.7082 - val_loss: 0.7531 - val_accuracy: 0.6756\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.4950 - accuracy: 0.8165 - val_loss: 0.8584 - val_accuracy: 0.6471\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.3211 - accuracy: 0.8800 - val_loss: 0.9215 - val_accuracy: 0.6437\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.2158 - accuracy: 0.9203 - val_loss: 1.0411 - val_accuracy: 0.6253\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1626 - accuracy: 0.9424 - val_loss: 1.0995 - val_accuracy: 0.6544\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1543 - accuracy: 0.9452 - val_loss: 1.1512 - val_accuracy: 0.6477\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1299 - accuracy: 0.9508 - val_loss: 1.2640 - val_accuracy: 0.6566\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.1211 - accuracy: 0.9572 - val_loss: 1.2679 - val_accuracy: 0.6437\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1034 - accuracy: 0.9594 - val_loss: 1.3519 - val_accuracy: 0.6482\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0919 - accuracy: 0.9633 - val_loss: 1.3687 - val_accuracy: 0.6348\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0808 - accuracy: 0.9667 - val_loss: 1.4782 - val_accuracy: 0.6437\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.6301 - accuracy: 0.6391\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 33ms/step - loss: 0.8805 - accuracy: 0.5929 - val_loss: 0.8079 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.7494 - accuracy: 0.6514 - val_loss: 0.7522 - val_accuracy: 0.6437\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.5312 - accuracy: 0.7804 - val_loss: 0.7779 - val_accuracy: 0.6566\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.3419 - accuracy: 0.8769 - val_loss: 0.8574 - val_accuracy: 0.6404\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.2397 - accuracy: 0.9107 - val_loss: 0.9666 - val_accuracy: 0.6337\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1855 - accuracy: 0.9331 - val_loss: 1.0017 - val_accuracy: 0.6510\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1549 - accuracy: 0.9410 - val_loss: 1.0983 - val_accuracy: 0.6544\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1370 - accuracy: 0.9477 - val_loss: 1.0742 - val_accuracy: 0.6309\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1277 - accuracy: 0.9505 - val_loss: 1.1754 - val_accuracy: 0.6544\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1111 - accuracy: 0.9536 - val_loss: 1.2287 - val_accuracy: 0.6521\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1011 - accuracy: 0.9558 - val_loss: 1.3181 - val_accuracy: 0.6516\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0905 - accuracy: 0.9608 - val_loss: 1.3284 - val_accuracy: 0.6264\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.4249 - accuracy: 0.6184\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 34ms/step - loss: 0.8696 - accuracy: 0.6086 - val_loss: 0.8138 - val_accuracy: 0.6163\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.7444 - accuracy: 0.6575 - val_loss: 0.7516 - val_accuracy: 0.6499\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.5257 - accuracy: 0.7902 - val_loss: 0.8008 - val_accuracy: 0.6482\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.3573 - accuracy: 0.8657 - val_loss: 0.9031 - val_accuracy: 0.6482\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.2557 - accuracy: 0.9133 - val_loss: 0.9462 - val_accuracy: 0.6521\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1969 - accuracy: 0.9267 - val_loss: 0.9825 - val_accuracy: 0.6432\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1719 - accuracy: 0.9370 - val_loss: 1.0434 - val_accuracy: 0.6376\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.1507 - accuracy: 0.9454 - val_loss: 1.1668 - val_accuracy: 0.6331\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1347 - accuracy: 0.9480 - val_loss: 1.1425 - val_accuracy: 0.6359\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1252 - accuracy: 0.9510 - val_loss: 1.2354 - val_accuracy: 0.6449\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1087 - accuracy: 0.9538 - val_loss: 1.2559 - val_accuracy: 0.6348\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.0997 - accuracy: 0.9578 - val_loss: 1.3741 - val_accuracy: 0.6292\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.3736 - accuracy: 0.6267\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 0.8460 - accuracy: 0.5895 - val_loss: 0.7680 - val_accuracy: 0.6393\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.6865 - accuracy: 0.6964 - val_loss: 0.7423 - val_accuracy: 0.6695\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.5129 - accuracy: 0.7868 - val_loss: 0.8120 - val_accuracy: 0.6493\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.3855 - accuracy: 0.8481 - val_loss: 1.0548 - val_accuracy: 0.6337\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.3011 - accuracy: 0.8808 - val_loss: 0.9457 - val_accuracy: 0.6488\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.2406 - accuracy: 0.9049 - val_loss: 1.0119 - val_accuracy: 0.6504\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.2078 - accuracy: 0.9203 - val_loss: 1.1313 - val_accuracy: 0.6359\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.1742 - accuracy: 0.9301 - val_loss: 1.2316 - val_accuracy: 0.6141\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.1514 - accuracy: 0.9407 - val_loss: 1.2704 - val_accuracy: 0.6538\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1400 - accuracy: 0.9438 - val_loss: 1.2785 - val_accuracy: 0.6504\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.1300 - accuracy: 0.9482 - val_loss: 1.3691 - val_accuracy: 0.6504\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.1203 - accuracy: 0.9519 - val_loss: 1.3755 - val_accuracy: 0.6353\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5502 - accuracy: 0.6060\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 45ms/step - loss: 0.8434 - accuracy: 0.6063 - val_loss: 0.7621 - val_accuracy: 0.6353\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.6747 - accuracy: 0.6970 - val_loss: 0.7315 - val_accuracy: 0.6633\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.5101 - accuracy: 0.7846 - val_loss: 0.7590 - val_accuracy: 0.6667\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.3994 - accuracy: 0.8394 - val_loss: 0.8739 - val_accuracy: 0.6421\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.3266 - accuracy: 0.8699 - val_loss: 0.9920 - val_accuracy: 0.6572\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.2780 - accuracy: 0.8867 - val_loss: 1.0117 - val_accuracy: 0.6471\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.2329 - accuracy: 0.9021 - val_loss: 1.0840 - val_accuracy: 0.6695\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.2021 - accuracy: 0.9180 - val_loss: 1.0884 - val_accuracy: 0.6242\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.1824 - accuracy: 0.9261 - val_loss: 1.2609 - val_accuracy: 0.6409\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1627 - accuracy: 0.9328 - val_loss: 1.2507 - val_accuracy: 0.6600\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.1485 - accuracy: 0.9393 - val_loss: 1.2508 - val_accuracy: 0.6527\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.1345 - accuracy: 0.9415 - val_loss: 1.3870 - val_accuracy: 0.6253\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.4060 - accuracy: 0.6200\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.8424 - accuracy: 0.6119 - val_loss: 0.8100 - val_accuracy: 0.6219\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.6931 - accuracy: 0.7017 - val_loss: 0.7716 - val_accuracy: 0.6622\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.5214 - accuracy: 0.7832 - val_loss: 0.8223 - val_accuracy: 0.6527\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.4015 - accuracy: 0.8321 - val_loss: 0.8070 - val_accuracy: 0.6633\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.3253 - accuracy: 0.8724 - val_loss: 0.9174 - val_accuracy: 0.6493\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.2810 - accuracy: 0.8906 - val_loss: 0.9552 - val_accuracy: 0.6577\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.2365 - accuracy: 0.9071 - val_loss: 1.0718 - val_accuracy: 0.6381\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.2086 - accuracy: 0.9163 - val_loss: 1.1032 - val_accuracy: 0.6454\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.1751 - accuracy: 0.9281 - val_loss: 1.3002 - val_accuracy: 0.6398\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.1649 - accuracy: 0.9340 - val_loss: 1.3030 - val_accuracy: 0.6051\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.1482 - accuracy: 0.9362 - val_loss: 1.5061 - val_accuracy: 0.6147\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 0.1345 - accuracy: 0.9404 - val_loss: 1.8805 - val_accuracy: 0.5554\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.9213 - accuracy: 0.5406\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 3s 49ms/step - loss: 0.8575 - accuracy: 0.5982 - val_loss: 0.7723 - val_accuracy: 0.6365\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.6428 - accuracy: 0.7224 - val_loss: 0.7818 - val_accuracy: 0.6588\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.3841 - accuracy: 0.8447 - val_loss: 0.8606 - val_accuracy: 0.6683\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.2477 - accuracy: 0.9068 - val_loss: 1.0336 - val_accuracy: 0.6639\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.1825 - accuracy: 0.9340 - val_loss: 1.2407 - val_accuracy: 0.6482\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.1487 - accuracy: 0.9488 - val_loss: 1.1666 - val_accuracy: 0.6404\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1245 - accuracy: 0.9524 - val_loss: 1.3539 - val_accuracy: 0.6605\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.1111 - accuracy: 0.9522 - val_loss: 1.5109 - val_accuracy: 0.6247\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.0964 - accuracy: 0.9608 - val_loss: 1.4058 - val_accuracy: 0.6477\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.0839 - accuracy: 0.9647 - val_loss: 1.4333 - val_accuracy: 0.6432\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.0839 - accuracy: 0.9639 - val_loss: 1.7141 - val_accuracy: 0.6398\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.8447 - accuracy: 0.6212\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 3s 50ms/step - loss: 0.8642 - accuracy: 0.5976 - val_loss: 0.7864 - val_accuracy: 0.6270\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.6418 - accuracy: 0.7188 - val_loss: 0.7519 - val_accuracy: 0.6465\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.4116 - accuracy: 0.8338 - val_loss: 0.8394 - val_accuracy: 0.6583\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.2672 - accuracy: 0.8965 - val_loss: 0.9676 - val_accuracy: 0.6421\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1954 - accuracy: 0.9250 - val_loss: 1.0722 - val_accuracy: 0.6572\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1631 - accuracy: 0.9365 - val_loss: 1.0684 - val_accuracy: 0.6499\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1455 - accuracy: 0.9421 - val_loss: 1.1909 - val_accuracy: 0.6432\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.1209 - accuracy: 0.9505 - val_loss: 1.3314 - val_accuracy: 0.6292\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1083 - accuracy: 0.9505 - val_loss: 1.5113 - val_accuracy: 0.6415\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.0936 - accuracy: 0.9538 - val_loss: 1.5637 - val_accuracy: 0.6398\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.0906 - accuracy: 0.9575 - val_loss: 1.6636 - val_accuracy: 0.6432\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.0857 - accuracy: 0.9580 - val_loss: 1.5673 - val_accuracy: 0.6174\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5638 - accuracy: 0.6256\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 3s 49ms/step - loss: 0.8562 - accuracy: 0.5988 - val_loss: 0.8058 - val_accuracy: 0.6180\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.6751 - accuracy: 0.7003 - val_loss: 0.7531 - val_accuracy: 0.6689\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.4126 - accuracy: 0.8338 - val_loss: 0.8249 - val_accuracy: 0.6544\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.2543 - accuracy: 0.9032 - val_loss: 0.9514 - val_accuracy: 0.6488\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.1973 - accuracy: 0.9264 - val_loss: 1.0482 - val_accuracy: 0.6611\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1710 - accuracy: 0.9354 - val_loss: 1.1244 - val_accuracy: 0.6493\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1412 - accuracy: 0.9424 - val_loss: 1.2928 - val_accuracy: 0.6258\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.1340 - accuracy: 0.9443 - val_loss: 1.1940 - val_accuracy: 0.6454\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1150 - accuracy: 0.9505 - val_loss: 1.2035 - val_accuracy: 0.6493\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1052 - accuracy: 0.9555 - val_loss: 1.4556 - val_accuracy: 0.6527\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.1011 - accuracy: 0.9555 - val_loss: 1.2884 - val_accuracy: 0.6370\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.0899 - accuracy: 0.9558 - val_loss: 1.5195 - val_accuracy: 0.6292\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5172 - accuracy: 0.6144\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 3s 54ms/step - loss: 0.8376 - accuracy: 0.5797 - val_loss: 0.7734 - val_accuracy: 0.6387\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.6644 - accuracy: 0.7090 - val_loss: 0.7451 - val_accuracy: 0.6566\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.4864 - accuracy: 0.7988 - val_loss: 0.8452 - val_accuracy: 0.6600\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.3756 - accuracy: 0.8478 - val_loss: 0.8425 - val_accuracy: 0.6600\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 2s 43ms/step - loss: 0.2893 - accuracy: 0.8931 - val_loss: 0.9976 - val_accuracy: 0.6292\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.2281 - accuracy: 0.9119 - val_loss: 1.0094 - val_accuracy: 0.6577\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 2s 43ms/step - loss: 0.1950 - accuracy: 0.9287 - val_loss: 1.1789 - val_accuracy: 0.6230\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 3s 52ms/step - loss: 0.1810 - accuracy: 0.9334 - val_loss: 1.1255 - val_accuracy: 0.6225\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.1566 - accuracy: 0.9424 - val_loss: 1.2084 - val_accuracy: 0.6655\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.1335 - accuracy: 0.9468 - val_loss: 1.2924 - val_accuracy: 0.6292\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.1343 - accuracy: 0.9510 - val_loss: 1.4535 - val_accuracy: 0.6504\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 2s 41ms/step - loss: 0.1219 - accuracy: 0.9533 - val_loss: 1.4106 - val_accuracy: 0.6432\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.5208 - accuracy: 0.6184\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 3s 55ms/step - loss: 0.8630 - accuracy: 0.5929 - val_loss: 0.7739 - val_accuracy: 0.6443\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 2s 43ms/step - loss: 0.6707 - accuracy: 0.7009 - val_loss: 0.8372 - val_accuracy: 0.5777\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 2s 41ms/step - loss: 0.4910 - accuracy: 0.7994 - val_loss: 0.8143 - val_accuracy: 0.6728\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.3851 - accuracy: 0.8481 - val_loss: 0.8779 - val_accuracy: 0.6298\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.2972 - accuracy: 0.8816 - val_loss: 1.0605 - val_accuracy: 0.6465\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.2532 - accuracy: 0.9012 - val_loss: 1.2427 - val_accuracy: 0.6264\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 2s 44ms/step - loss: 0.2170 - accuracy: 0.9093 - val_loss: 1.2823 - val_accuracy: 0.6633\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.1897 - accuracy: 0.9239 - val_loss: 1.2067 - val_accuracy: 0.6292\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.1678 - accuracy: 0.9365 - val_loss: 1.2977 - val_accuracy: 0.6337\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.1507 - accuracy: 0.9384 - val_loss: 1.3641 - val_accuracy: 0.6242\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 2s 43ms/step - loss: 0.1364 - accuracy: 0.9440 - val_loss: 1.4161 - val_accuracy: 0.6393\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 1.4712 - accuracy: 0.6240\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 3s 54ms/step - loss: 0.8461 - accuracy: 0.6032 - val_loss: 0.7558 - val_accuracy: 0.6365\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 0.6575 - accuracy: 0.7037 - val_loss: 0.9107 - val_accuracy: 0.6516\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 2s 41ms/step - loss: 0.4969 - accuracy: 0.7918 - val_loss: 0.7778 - val_accuracy: 0.6706\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 2s 43ms/step - loss: 0.3726 - accuracy: 0.8447 - val_loss: 0.9892 - val_accuracy: 0.6353\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 2s 43ms/step - loss: 0.3005 - accuracy: 0.8788 - val_loss: 0.9557 - val_accuracy: 0.6381\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 2s 41ms/step - loss: 0.2464 - accuracy: 0.9024 - val_loss: 1.0237 - val_accuracy: 0.6465\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 2s 43ms/step - loss: 0.2150 - accuracy: 0.9172 - val_loss: 1.1665 - val_accuracy: 0.6622\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.1891 - accuracy: 0.9292 - val_loss: 1.2472 - val_accuracy: 0.6465\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 2s 43ms/step - loss: 0.1631 - accuracy: 0.9373 - val_loss: 1.1199 - val_accuracy: 0.6471\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 2s 43ms/step - loss: 0.1541 - accuracy: 0.9390 - val_loss: 1.2441 - val_accuracy: 0.6527\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 2s 40ms/step - loss: 0.1377 - accuracy: 0.9446 - val_loss: 1.3533 - val_accuracy: 0.6443\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.3768 - accuracy: 0.6374\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 5s 83ms/step - loss: 0.8353 - accuracy: 0.5926 - val_loss: 0.7712 - val_accuracy: 0.6365\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 4s 74ms/step - loss: 0.6153 - accuracy: 0.7278 - val_loss: 0.7585 - val_accuracy: 0.6426\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.3925 - accuracy: 0.8489 - val_loss: 0.9022 - val_accuracy: 0.6275\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 4s 75ms/step - loss: 0.2473 - accuracy: 0.9077 - val_loss: 1.0223 - val_accuracy: 0.6477\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 4s 72ms/step - loss: 0.1720 - accuracy: 0.9410 - val_loss: 1.2657 - val_accuracy: 0.6292\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 4s 74ms/step - loss: 0.1508 - accuracy: 0.9463 - val_loss: 1.1308 - val_accuracy: 0.6465\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 4s 74ms/step - loss: 0.1198 - accuracy: 0.9564 - val_loss: 1.3208 - val_accuracy: 0.6197\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.1109 - accuracy: 0.9591 - val_loss: 1.3673 - val_accuracy: 0.6471\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 4s 71ms/step - loss: 0.1044 - accuracy: 0.9591 - val_loss: 1.5453 - val_accuracy: 0.6370\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 4s 75ms/step - loss: 0.0833 - accuracy: 0.9661 - val_loss: 1.4639 - val_accuracy: 0.6387\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 4s 72ms/step - loss: 0.0870 - accuracy: 0.9661 - val_loss: 1.4943 - val_accuracy: 0.6465\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.0682 - accuracy: 0.9695 - val_loss: 1.6700 - val_accuracy: 0.6353\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 1.7803 - accuracy: 0.6407\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 5s 84ms/step - loss: 0.8350 - accuracy: 0.6035 - val_loss: 0.7598 - val_accuracy: 0.6393\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.6014 - accuracy: 0.7423 - val_loss: 0.7786 - val_accuracy: 0.6594\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 4s 74ms/step - loss: 0.3662 - accuracy: 0.8565 - val_loss: 0.8713 - val_accuracy: 0.6449\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 4s 72ms/step - loss: 0.2410 - accuracy: 0.9127 - val_loss: 1.0281 - val_accuracy: 0.6398\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 4s 71ms/step - loss: 0.1904 - accuracy: 0.9275 - val_loss: 1.1361 - val_accuracy: 0.6488\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 4s 75ms/step - loss: 0.1677 - accuracy: 0.9379 - val_loss: 1.0901 - val_accuracy: 0.6499\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.1498 - accuracy: 0.9449 - val_loss: 1.2584 - val_accuracy: 0.6527\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.1183 - accuracy: 0.9505 - val_loss: 1.3447 - val_accuracy: 0.6393\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.1031 - accuracy: 0.9508 - val_loss: 1.3607 - val_accuracy: 0.6376\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 4s 75ms/step - loss: 0.1011 - accuracy: 0.9550 - val_loss: 1.6369 - val_accuracy: 0.6454\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.0889 - accuracy: 0.9580 - val_loss: 1.6793 - val_accuracy: 0.6393\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.7471 - accuracy: 0.6385\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 5s 83ms/step - loss: 0.8284 - accuracy: 0.6167 - val_loss: 0.7523 - val_accuracy: 0.6432\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 4s 74ms/step - loss: 0.6115 - accuracy: 0.7373 - val_loss: 0.8046 - val_accuracy: 0.6650\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 4s 74ms/step - loss: 0.3884 - accuracy: 0.8458 - val_loss: 0.8641 - val_accuracy: 0.6477\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.2534 - accuracy: 0.9060 - val_loss: 0.9806 - val_accuracy: 0.6353\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.1934 - accuracy: 0.9309 - val_loss: 1.0316 - val_accuracy: 0.6387\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 4s 72ms/step - loss: 0.1645 - accuracy: 0.9382 - val_loss: 1.1489 - val_accuracy: 0.6477\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 4s 75ms/step - loss: 0.1428 - accuracy: 0.9443 - val_loss: 1.1830 - val_accuracy: 0.6381\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.1469 - accuracy: 0.9435 - val_loss: 1.2318 - val_accuracy: 0.6488\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 4s 74ms/step - loss: 0.1152 - accuracy: 0.9530 - val_loss: 1.5246 - val_accuracy: 0.6359\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 4s 72ms/step - loss: 0.0970 - accuracy: 0.9569 - val_loss: 1.4836 - val_accuracy: 0.6320\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 4s 73ms/step - loss: 0.1062 - accuracy: 0.9541 - val_loss: 1.6037 - val_accuracy: 0.6370\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.6456 - accuracy: 0.6329\n",
            "Epoch 1/50\n",
            "84/84 [==============================] - 4s 44ms/step - loss: 0.8355 - accuracy: 0.6072 - val_loss: 0.7439 - val_accuracy: 0.6449\n",
            "Epoch 2/50\n",
            "84/84 [==============================] - 3s 36ms/step - loss: 0.6336 - accuracy: 0.7230 - val_loss: 0.7383 - val_accuracy: 0.6728\n",
            "Epoch 3/50\n",
            "84/84 [==============================] - 3s 37ms/step - loss: 0.4376 - accuracy: 0.8284 - val_loss: 0.7725 - val_accuracy: 0.6711\n",
            "Epoch 4/50\n",
            "84/84 [==============================] - 3s 37ms/step - loss: 0.3204 - accuracy: 0.8756 - val_loss: 0.9282 - val_accuracy: 0.6751\n",
            "Epoch 5/50\n",
            "84/84 [==============================] - 3s 38ms/step - loss: 0.2474 - accuracy: 0.9077 - val_loss: 0.9776 - val_accuracy: 0.6661\n",
            "Epoch 6/50\n",
            "84/84 [==============================] - 3s 39ms/step - loss: 0.2040 - accuracy: 0.9239 - val_loss: 1.0498 - val_accuracy: 0.6650\n",
            "Epoch 7/50\n",
            "84/84 [==============================] - 3s 37ms/step - loss: 0.1743 - accuracy: 0.9297 - val_loss: 1.1556 - val_accuracy: 0.6303\n",
            "Epoch 8/50\n",
            "84/84 [==============================] - 3s 37ms/step - loss: 0.1541 - accuracy: 0.9373 - val_loss: 1.2546 - val_accuracy: 0.6616\n",
            "Epoch 9/50\n",
            "84/84 [==============================] - 3s 37ms/step - loss: 0.1360 - accuracy: 0.9379 - val_loss: 1.2173 - val_accuracy: 0.6555\n",
            "Epoch 10/50\n",
            "84/84 [==============================] - 3s 38ms/step - loss: 0.1302 - accuracy: 0.9399 - val_loss: 1.1177 - val_accuracy: 0.6566\n",
            "Epoch 11/50\n",
            "84/84 [==============================] - 3s 36ms/step - loss: 0.1168 - accuracy: 0.9450 - val_loss: 1.4904 - val_accuracy: 0.6644\n",
            "Epoch 12/50\n",
            "84/84 [==============================] - 3s 37ms/step - loss: 0.1069 - accuracy: 0.9526 - val_loss: 1.5066 - val_accuracy: 0.6493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu',\n",
              " 'metrics': 'accuracy',\n",
              " 'nodes': 128,\n",
              " 'optimizer': 'adam',\n",
              " 'output': 3,\n",
              " 'vocab': 21444}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ3Ioc7xJtNQ",
        "outputId": "31ed8e29-8b3e-4022-e507-b50f95d281c1"
      },
      "source": [
        "model = models.Sequential()\r\n",
        "model.add(layers.Embedding(21444, 128))\r\n",
        "model.add(layers.Bidirectional(layers.LSTM(128)))\r\n",
        "model.add(layers.Dense(64, activation='relu'))\r\n",
        "model.add(layers.Dense(32, activation='relu'))\r\n",
        "model.add(layers.Dense(3, activation='softmax'))\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2, callbacks=early_stopping)\r\n",
        "model.save('bid.h5')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "101/101 [==============================] - 4s 44ms/step - loss: 0.8504 - accuracy: 0.5954 - val_loss: 0.8064 - val_accuracy: 0.6128\n",
            "Epoch 2/50\n",
            "101/101 [==============================] - 4s 36ms/step - loss: 0.6516 - accuracy: 0.7084 - val_loss: 0.7214 - val_accuracy: 0.6942\n",
            "Epoch 3/50\n",
            "101/101 [==============================] - 4s 37ms/step - loss: 0.4233 - accuracy: 0.8289 - val_loss: 0.7971 - val_accuracy: 0.6762\n",
            "Epoch 4/50\n",
            "101/101 [==============================] - 4s 37ms/step - loss: 0.3011 - accuracy: 0.8829 - val_loss: 0.8375 - val_accuracy: 0.7023\n",
            "Epoch 5/50\n",
            "101/101 [==============================] - 4s 36ms/step - loss: 0.2423 - accuracy: 0.9042 - val_loss: 0.9365 - val_accuracy: 0.6768\n",
            "Epoch 6/50\n",
            "101/101 [==============================] - 4s 37ms/step - loss: 0.2104 - accuracy: 0.9123 - val_loss: 0.9887 - val_accuracy: 0.6899\n",
            "Epoch 7/50\n",
            "101/101 [==============================] - 4s 36ms/step - loss: 0.1767 - accuracy: 0.9246 - val_loss: 1.0431 - val_accuracy: 0.6712\n",
            "Epoch 8/50\n",
            "101/101 [==============================] - 4s 37ms/step - loss: 0.1522 - accuracy: 0.9316 - val_loss: 1.2334 - val_accuracy: 0.6892\n",
            "Epoch 9/50\n",
            "101/101 [==============================] - 4s 36ms/step - loss: 0.1412 - accuracy: 0.9350 - val_loss: 1.1037 - val_accuracy: 0.6812\n",
            "Epoch 10/50\n",
            "101/101 [==============================] - 4s 37ms/step - loss: 0.1349 - accuracy: 0.9364 - val_loss: 1.1021 - val_accuracy: 0.6849\n",
            "Epoch 11/50\n",
            "101/101 [==============================] - 4s 36ms/step - loss: 0.1214 - accuracy: 0.9406 - val_loss: 1.3666 - val_accuracy: 0.6774\n",
            "Epoch 12/50\n",
            "101/101 [==============================] - 4s 36ms/step - loss: 0.1148 - accuracy: 0.9420 - val_loss: 1.6020 - val_accuracy: 0.6793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGHxbJKQLTgw",
        "outputId": "b744e4a7-9163-4611-e8ed-800376783008"
      },
      "source": [
        "model.evaluate(X_test, y_test, batch_size=64)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 6ms/step - loss: 1.8336 - accuracy: 0.6532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.8336397409439087, 0.6532438397407532]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbEAJFJ8L-pw",
        "outputId": "46fa55e5-3bc8-4af8-b0ec-23e1478c135e"
      },
      "source": [
        "model.evaluate(X_train, y_train, batch_size=64)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "126/126 [==============================] - 1s 5ms/step - loss: 0.3897 - accuracy: 0.9004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.38971319794654846, 0.9003978967666626]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "2J8wBZfoLgG9",
        "outputId": "8f02f79e-86be-4393-aee1-20bba0033708"
      },
      "source": [
        "dh.ViewAccuracy(history, 12)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "5hZxxux4L0PY",
        "outputId": "76366e15-423e-437c-92fe-1dbcb6c3d109"
      },
      "source": [
        "dh.ViewLoss(history, 12)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2NkRKsDHqzD"
      },
      "source": [
        "I was planning on doing another gridsearch on my RNN building function to test out LSTM's and GRU's of varying sizes. However I'm going to do something about class imbalances first. Although I deleted these cells for some reason, I realized that the only way to stop the model from overfitting was to use l1 regularization. However, train and validation accuracy just stay around 60% which is interesting because ~60% of the data is of one class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNV59msdIxTz",
        "outputId": "7007773a-b1f2-4187-b062-35ca8d220b98"
      },
      "source": [
        "df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "No emotion toward brand or product    5388\n",
              "Positive emotion                      2978\n",
              "Negative emotion                       570\n",
              "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww5x8HJQI8j4"
      },
      "source": [
        "I went back into my network building functions to parameritize metrics. if you look at the code it's already fixed but just note that accuracy was originally hardcoded as the metric for the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBfp4-E9a5k4",
        "outputId": "ed6cdbb3-ed0f-488e-e620-6ba030ef89d9"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\r\n",
        "\r\n",
        "weights = compute_class_weight('balanced',\r\n",
        "                                np.unique(df['is_there_an_emotion_directed_at_a_brand_or_product']),\r\n",
        "                                df['is_there_an_emotion_directed_at_a_brand_or_product'])\r\n",
        "\r\n",
        "weights_dict = dict(zip(range(3), weights))\r\n",
        "weights_dict"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 5.2257309941520464, 1: 0.5528334570650829, 2: 1.0002238638907543}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6_vD43dI63k",
        "outputId": "ffffba4a-bd6c-4be7-972a-30064b4c531c"
      },
      "source": [
        "from keras.metrics import Recall, Precision\r\n",
        "\r\n",
        "model = models.Sequential()\r\n",
        "model.add(layers.Embedding(21444, 128))\r\n",
        "model.add(layers.Bidirectional(layers.LSTM(128)))\r\n",
        "model.add(layers.Dense(64, activation='relu'))\r\n",
        "model.add(layers.Dense(32, activation='relu'))\r\n",
        "model.add(layers.Dense(3, activation='softmax'))\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2, callbacks=early_stopping, class_weight=weights_dict)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "101/101 [==============================] - 4s 40ms/step - loss: 1.0404 - accuracy: 0.5024 - val_loss: 0.9062 - val_accuracy: 0.5500\n",
            "Epoch 2/50\n",
            "101/101 [==============================] - 3s 35ms/step - loss: 0.6891 - accuracy: 0.6771 - val_loss: 0.8535 - val_accuracy: 0.5960\n",
            "Epoch 3/50\n",
            "101/101 [==============================] - 4s 35ms/step - loss: 0.4141 - accuracy: 0.8062 - val_loss: 0.9926 - val_accuracy: 0.5966\n",
            "Epoch 4/50\n",
            "101/101 [==============================] - 3s 35ms/step - loss: 0.2780 - accuracy: 0.8610 - val_loss: 0.9425 - val_accuracy: 0.6476\n",
            "Epoch 5/50\n",
            "101/101 [==============================] - 4s 35ms/step - loss: 0.2178 - accuracy: 0.8889 - val_loss: 1.0205 - val_accuracy: 0.6408\n",
            "Epoch 6/50\n",
            "101/101 [==============================] - 4s 35ms/step - loss: 0.1837 - accuracy: 0.9044 - val_loss: 1.0740 - val_accuracy: 0.6464\n",
            "Epoch 7/50\n",
            "101/101 [==============================] - 3s 34ms/step - loss: 0.1644 - accuracy: 0.9117 - val_loss: 1.0860 - val_accuracy: 0.6526\n",
            "Epoch 8/50\n",
            "101/101 [==============================] - 4s 35ms/step - loss: 0.1597 - accuracy: 0.9187 - val_loss: 1.1064 - val_accuracy: 0.6600\n",
            "Epoch 9/50\n",
            "101/101 [==============================] - 3s 34ms/step - loss: 0.1310 - accuracy: 0.9241 - val_loss: 1.2459 - val_accuracy: 0.6576\n",
            "Epoch 10/50\n",
            "101/101 [==============================] - 3s 35ms/step - loss: 0.1210 - accuracy: 0.9321 - val_loss: 1.3300 - val_accuracy: 0.6234\n",
            "Epoch 11/50\n",
            "101/101 [==============================] - 3s 34ms/step - loss: 0.1150 - accuracy: 0.9322 - val_loss: 1.3210 - val_accuracy: 0.6234\n",
            "Epoch 12/50\n",
            "101/101 [==============================] - 3s 34ms/step - loss: 0.1182 - accuracy: 0.9282 - val_loss: 1.4148 - val_accuracy: 0.6227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "RF_76Njde8KO",
        "outputId": "76facd7b-a00b-4a91-985b-f5f5ca752713"
      },
      "source": [
        "dh.ViewAccuracy(history, 12)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "rW149xv7fFL0",
        "outputId": "64be21b9-6d04-45c9-aecd-f4c3aa4f5c19"
      },
      "source": [
        "dh.ViewLoss(history, 12)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3yV5f3/8dcnJyc7YSSBQMKSPQUNOHCgKAUcuNGq1Trosmpta+23/XXY5WhttbUqdbdWi1pHFQUH4AIkuNh7hRlWSIDs6/fHfQIBQwhyTu6cnPfz8TiPc+5x7vM5Gu73ua/7uq/bnHOIiEjsivO7ABER8ZeCQEQkxikIRERinIJARCTGKQhERGJcvN8FHKmsrCzXtWtXv8sQEYkqc+fO3eqcy65vWdQFQdeuXSkoKPC7DBGRqGJmaw61TE1DIiIxTkEgIhLjFAQiIjEu6s4R1KeyspLCwkLKysr8LiXikpKSyMvLIxgM+l2KiLQQLSIICgsLSU9Pp2vXrpiZ3+VEjHOObdu2UVhYSLdu3fwuR0RaiBbRNFRWVkZmZmaLDgEAMyMzMzMmjnxEpOm0iCAAWnwI1IqV7ykiTSdiQWBmj5vZFjObf5j1hppZlZldEqlaRESiWsUe+PB+WDsrIpuP5BHBk8DohlYwswBwNzA1gnVE3M6dO/n73/9+xO8bO3YsO3fujEBFItIiVJXD7Efg/mPhrV/A0jcj8jERCwLn3HvA9sOs9n3gRWBLpOpoCocKgqqqqgbfN3nyZFq3bh2pskQkWlVXQsET8MBx8MbtkNULvvkmnPWriHycb72GzCwXuBA4Axh6mHUnABMAOnfuHPnijtAdd9zBihUrGDx4MMFgkKSkJNq0acPixYtZunQpF1xwAevWraOsrIxbbrmFCRMmAPuHyygtLWXMmDGccsopfPTRR+Tm5vLKK6+QnJzs8zcTkSZVUw1fTIIZd8GO1ZA3FC54ELqdDhE8P+hn99G/AD9xztUc7gSoc24iMBEgPz+/wXtr/vp/C1i4YVfYigTo1zGDX57X/5DL77rrLubPn89nn33G9OnTOeecc5g/f/6+Lp6PP/44bdu2Ze/evQwdOpSLL76YzMzMA7axbNkynn32Wf7xj39w2WWX8eKLL3LVVVeF9XuISDNVUwMLX4bpf4CtSyFnEHz9eeh5dkQDoJafQZAPPBcKgSxgrJlVOede9rGmsBg2bNgB/fwfeOABXnrpJQDWrVvHsmXLvhQE3bp1Y/DgwQAcf/zxrF69usnqFRGfOAdL3oBpv4PN8yG7L1z2T+h7XpMEQC3fgsA5t29PaWZPAq+FIwQa+uXeVFJTU/e9nj59Om+//TYzZ84kJSWFESNG1HsdQGJi4r7XgUCAvXv3NkmtIuID52DFO/Du72DDJ9D2GLjoURhwEcQFmryciAWBmT0LjACyzKwQ+CUQBHDOPRypz/VDeno6JSUl9S4rLi6mTZs2pKSksHjxYmbNikz3LxGJEqs/gHd/C2tnQqvOMO5BGHQ5BPxroInYJzvnrjiCda+NVB1NITMzk+HDhzNgwACSk5Np3779vmWjR4/m4Ycfpm/fvvTu3ZsTTzzRx0pFxDfr5sC038LK6ZDeAc75Ewz5BsQn+F0Z5lyD516bnfz8fHfwjWkWLVpE3759faqo6cXa9xWJahs/h2m/964BSMmCU2+D/Osg2LS9As1srnMuv75lLWLQORGRZmfLIi8AFr0KSa1g5C9g2LcgMc3vyr5EQSAiEk7bVsD0u2De85CQBqf/BE78LiQ334tHFQQiIuGwcy3MuAc++zcEEmD4zXDyLZCaefj3+kxBICLRzzlY/T4sfAXigl7zS0Kq94s8Ia3OdPqXlx3tydpdG+H9P8HcJ72+/8MmwCk/gPT2h31rc6EgEJHoVVPttcF/eD9s+NTbsVsAKkrA1TRuG4GE/SGRkBoKilBYJKYfIlDSvGUrp8OcR6GmCoZcDaf9CFrlRfQrR4KCQESiT+Verwnmo7/CjlXQtjuc+xc49goIJnlHCFVlUF7qhULF7tDr3d503df7loUetctKN9eZLoXqii/XYXHeNQCn3w5to/eugQoCH6SlpVFaWup3GSLRZ892KHjMG5p5dxHkHg9n3wl9zjnwilwzr3tmMBnIDs9nV1WEwmL3/oBIawdtuoRn+z5SEIhI87dzHcz6O8x9Cip3Q89RMPwW6DK86cbkiU+A+LaQ0rZpPq8JKQjC4I477qBTp05873vfA+BXv/oV8fHxTJs2jR07dlBZWclvf/tbxo0b53OlIlFm8wL48AGY/4I3PeASrzdOe//HFGtJWl4QvHEHbJoX3m3mDIQxdx1y8fjx47n11lv3BcGkSZOYMmUKN998MxkZGWzdupUTTzyR888/X/ccFjkc52DNh94J4GVTIZjq9cQ58bvQupPf1bVILS8IfDBkyBC2bNnChg0bKCoqok2bNuTk5PCDH/yA9957j7i4ONavX8/mzZvJycnxu1yR5qmmGha/Dh/+BdbP9YZjOOPnMPT6Ftkc05y0vCBo4Jd7JF166aW88MILbNq0ifHjx/PMM89QVFTE3LlzCQaDdO3atd7hp0ViXmUZfPGc1wNo23Jo09UbkG3wlU0+Hk+sanlB4JPx48dz4403snXrVmbMmMGkSZNo164dwWCQadOmsWbNGr9LFGle9u70egDNehh2b4EOg+HSJ6Hv+b6MyR/LFARh0r9/f0pKSsjNzaVDhw5ceeWVnHfeeQwcOJD8/Hz69Onjd4kizUPx+lAPoCe9bpjdR3o9gLqd1qR35ZL9FARhNG/e/pPUWVlZzJw5s971dA2BxKQti70TwPMmeSeEB1wEJ98MHQb5XVnMUxCISOQ4B2tneSeAl74J8cmQfz2c9F3vXIA0CwoCEYmMnevgpW/Dmg8guS2M+CkMvTEqRuOMNS0mCJxzMdFHP9ruKCcxatnb8N8boLoKRt8Nx13tDdYmzVKLCIKkpCS2bdtGZmZmiw4D5xzbtm0jKSnJ71JE6ldT7d2U5b17vat/L3saMrv7XZUcRosIgry8PAoLCykqKvK7lIhLSkoiLy/6hrmVGFBaBC9eD6tmeNcAjP0jJKT4XZU0QosIgmAwSLdu0TsErEjUWzsLnv8m7N0O5//NawqSqBEXqQ2b2eNmtsXM5h9i+ZVm9oWZzTOzj8zs2EjVIiIR4hzMfBCePAfiE+H6txQCUShiQQA8CYxuYPkq4HTn3EDgN8DECNYiIuFWVgyTroYp/we9RsOE6bomIEpFrGnIOfeemXVtYPlHdSZnAWr4FokWm+bBpG/AjjUw6rdw0k26KjiKNZdzBNcDbxxqoZlNACYAdO7cualqEpH6fPoMvH4bJLWGa1+DLif7XZEcJd+DwMzOwAuCUw61jnNuIqGmo/z8fHWkF/FD5V6Y/GP49J/Q9VS45HHvVo0S9XwNAjMbBDwKjHHObfOzFhFpwLYVMOka2DwPTv0RnPF/GiG0BfEtCMysM/Bf4Grn3FK/6hCRw1j0P3j5u2Bx8PVJ0OtrflckYRaxIDCzZ4ERQJaZFQK/BIIAzrmHgV8AmcDfQ1cDVznn8iNVj4gcoepKeOfX3g1jOg6BS5+CNl38rkoiIJK9hq44zPIbgBsi9fkichR2bYQXvglrZ8LQG+Brv/euE5AWyfeTxSLSzKyc4Q0VUbEbLnoUBl3qd0USYQoCEfHU1MAH98G030FmD7jmNWinO+vFAgWBiMCe7fDSt2DZVBhwMZz3ACSm+V2VNBEFgUisWz8XJl0LJRu9EUOH3qCrhGOMgkAkVjkHcx71xgpKaw/XTYG84/2uSnygIBCJReWl8NqtMO956HE2XDQRUtr6XZX4REEgEmuKlsB/roZty+DMn8MpP4S4SA5ELM2dgkCkpaqphtItsGu99yheDzvXwidPQzAZrn4Jjhnhd5XSDCgIRKJRTQ3s2QrFhaEd/Yb9r4tD0yUboKbqwPfFJ3mjhY57EDI6+lO7NDsKApHmxjnYs63OTn39ga+LC70ePtUVB74vkODt3DPyoMtJkJHrTbfK8163yoPkNuoRJF+iIBCJNOegohT27oSynfufy4q913u3H/iLftcGqCo7cBtxQcjo4O3Q84ZCq1xvh5/Rcf/r1Czt5OUrURCINEZNDZTvOnBHfsBzccPLXPWht20BSO/g7dA7HAu9x+7/FZ+R681PbacTuhIxCgKRWnt3wPJ3YMW73q/zA3697wIauCeSBSC5tXfXruTWXhNM227edFKrA5fte27lvU7M0E5efKUgkNjlHBQthqVTvKEV1s7yfrknt/XG2klrD1m99++8D7lDbw0JqWqWkailIJDYUrkXVn8AS9+EpVOheK03P2cgnPID76Yrucfr7lsSUxQE0vIVF+7/1b9yBlTthWCK14f+1Nug5yivHV4kRikIpOWpqYbCOft3/pvne/Nbd4HjroaeX4Oup0Awyd86RZqJmAmCxZt2MXHGSn5/0UCSgjrsb3H2bPdO8i6dAsvf8k78WgA6nwRn3+nt/LN7qx1fpB4xEwTbSiv476frGZjXim8O7+Z3OXK0nIMti7y2/mVTYd1scDWQkunt9HuNgu4jvZO5ItKgmAmC4T2yOOmYTB6ctoLxQzuRkhAzX73lqNwLq97b3+RTvM6bnzMQTrkNeo2G3ON0olfkCMXU3vBHX+vFxQ/N5KmP1vCdEd39Lkcao6ocFrwM81+EVTO8K25rT/Se9iPvRK/GzBE5KhELAjN7HDgX2OKcG1DPcgPuB8YCe4BrnXOfRKoegOO7tOWM3tk8PGMFV57YmYykYCQ/To5GySYoeNx77C6C1p3huGu8Jp8uOtErEk6RPCJ4Evgb8PQhlo8BeoYeJwAPhZ4j6oejenPuXz/gsfdX8YOze0X64+RIFRbA7IdhwUte75+eo+CEb8ExZ+jqW5EIiVgQOOfeM7OuDawyDnjaOeeAWWbW2sw6OOc2RqomgAG5rRgzIIfHPljFtSd3pU1qQiQ/Thqjtvln9sOw4RNvyIVhE7x752aqCU8k0vz8iZULrKszXRia9yVmNsHMCsysoKio6Kg/+Laze7G7ooqH31tx1NtqNjYvhB2r/a7iyJRsgml/gD8PgJcmQHmJd/P02xbC6D8oBESaSFScLHbOTQQmAuTn5zcw8lfj9GyfzgWDc3nqo9VcP7wb7TKiuL25uhLe+TV89Fdvuu0xXrfJHiO9i6YS0/2trz6FBTD7kVDzT6XX3VPNPyK+8TMI1gOd6kznheY1iVvP6smrn2/gwWnL+fW4L53Ljg67NsAL18HamZB/HWT38UbP/OwZmPMPbwz7TidA9zO8YMg51r8dbVUFLAw1/6yfCwnpXtPPsBv1y1/EZ34GwavATWb2HN5J4uJInx+oq0tmKpfl5/Hvj9dy42nHkNcmpak+OjxWTocXrvf61l/0KAy61Jt/wre8Nvd1s/cPqfzub7xHSqb3q7vHSO85o0Pk6yzZDHOf8Hr/lG72RvUccy8MvqJ5Hq2IxCDzztVGYMNmzwIjgCxgM/BLIAjgnHs41H30b8BovO6j33TOFRxuu/n5+a6g4LCrNcqGnXsZce90LhySy92XDArLNiOupgbe/xNM+x1k9YLx//SGTmhI6RYvOGqDYfcWb367/vuPFjqfHN4umYVz6/T+qazT++dMNf+I+MDM5jrn8utdFqkgiJRwBgHAr/+3gKdnruHt206nW1Zq2LYbEbu3eSdVl78NAy+Dc/8MiWlHtg3nvEHYakNh7Uzv3rfxSdBluBcK3c/0mpmOdFyeqgpY+Eqo+afAa/4ZcqXXA0jNPyK+UhA0oKiknNPumcao/u25//IhYdtu2BUWwKRrvF/zo+/yzgmEYwC1it2w5qNQMLwDW5d689M7eoHQ40yvGSml7aG3UboFCp6Agsf2N/8M+5aaf0SakYaCICp6DUVSdnoi1w7vysMzVvDdET3ondPMdlzOwccTYcrPvDb966dCxzAGVkIq9DzbewDsXOcdKax4Fxa/Bp/9CzDvM2uPFvKGQiDonfSd/QjM/6/X/NPjbDjh2946av4RiRoxf0QAsHNPBafePY2Te2TyyNX1BqY/ynbBq9/3etv0Gg0XPuzdC7ep1FTD+k9CwfCOd1Tiqr0mn1Z5ULRof/PP0Bshq0fT1SYiR0RHBIfROiWBG049hj+/vZQvCncyKK8ZDF28eQFM+gZsXwln/QpOvqXpf2XHBaDTUO8x4ifejdxXveeFQtFSGHMPHHsFJGU0bV0iElY6IggpKavktHumMSivNU9dNyzs2z8in/0bXrvN28Fe8rh3YZiIyFFo6IhADbkh6UlBvn16d2YsLWLO6u3+FFFZ5jUFvfwdyMuHb72vEBCRiFMQ1PGNk7qSnZ7IvVOW0ORHSttXwmNnwSdPw6k/hKtfhvT2TVuDiMSk2AmC8lLv1oYN7OCTEwLcdEYPPl61nQ+Wb2262ha9Bo+M8HrsfH0SjPwFBHT6RkSaRuwEwbIp8PcT4S+DvPb3JW96fegPcvmwTuS2TuaPU5dG/qigutLrFvqfKyHzGPjWe9Dra5H9TBGRg8ROEHQ9Fc79i3d/28+fg2fHw93d4J8XwayHYZs3JHVifICbR/bg83U7eXvRlsjVs2sDPHkuzPybN/jadVOgTZfIfZ6IyCHEZq+hqnLvatplb3k3Qd+2zJvftjv0HEV197MY83I1cQnJTL75VOLiwnAFb111B4w7/wEYeEl4ty8ichANMXE421fCsre9UFj9PlSVURVIZlpFX9offx6DRlwKrTsdfjuHU3fAuOzecNnThx8wTkQkDBQER6JiD6z+ALdsKpsLXiHH1Y7U2S80FMMob4z/wBHe+P7gAePO+4s3vIOISBNQEHxFU+Zv5J5n/sd9gzdzbNnHsGamN6ZOYoY3fHPPUdDjLEjPaXhD6+bA89d6A8aNuRuO/2Z4BowTEWkkDTHxFY3qn8ODuf343uoevPvDn5FQVQqrZnhNSMve8oZcBuhwrBcKPUdB7vHe0AzgdVWd/QhM/XlkBowTEQkDHREcxoylRVzz+Mf8Zlx/rj6p6/4FteP614bCutngaiC5rTdKZ4+zYcnk0IBxY+DCh5p2wDgRkTrUNHQUnHNc9shM1mzbw3u3n0FSMFD/inu2w8ppsHQqLH8L9mwDC3gXh518s4ZlFhFfqWnoKJgZPxrVm/ETZ/HPmWu48bRj6l8xpS0MuNh71NTAhk+9u4epV5CINHP6mdoIJxyTyak9s3hoxgpKy6sO/4a4OMg7XiEgIlFBQdBIPxzVm+27K3jig1V+lyIiElYKgkYa3Kk1Z/drz8T3V1K8p9LvckREwkZBcARuO7sXpeVVTHx/hd+liIiETUSDwMxGm9kSM1tuZnfUs7yzmU0zs0/N7AszGxvJeo5W3w4ZnDuoI098uJqtpeV+lyMiEhYRCwIzCwAPAmOAfsAVZtbvoNV+Dkxyzg0BLgf+Hql6wuXWs3pSVlnNQ9N1VCAiLUOjgsDMbjGzDPM8ZmafmNmow7xtGLDcObfSOVcBPAeMO2gdB9Te+bwVsOFIivdD9+w0Lj4uj3/OWsPG4r1+lyMictQae0RwnXNuFzAKaANcDdx1mPfkAuvqTBeG5tX1K+AqMysEJgPfr29DZjbBzArMrKCoqKiRJUfOzSN74pzjb+8u97sUEZGj1tggqB0hbSzwT+fcgjrzjsYVwJPOubzabZvZl2pyzk10zuU75/Kzs7PD8LFHp1PbFC4f2pn/zFnH2m17/C5HROSoNDYI5prZVLyd9RQzSwdqDvOe9UDdQfzzQvPquh6YBOCcmwkkAVmNrMlXN53Zg0Cccf87y/wuRUTkqDQ2CK4H7gCGOuf2AEHgm4d5zxygp5l1M7MEvJPBrx60zlpgJICZ9cULAv/bfhqhfUYS3zipCy99WsjyLSV+lyMi8pU1NghOApY453aa2VV4vX2KG3qDc64KuAmYAizC6x20wMzuNLPzQ6v9ELjRzD4HngWudVE0Ct63T+9OcjDAn9/SUYGIRK/GDjr3EHCsmR2Lt/N+FHgaOL2hNznnJuOdBK477xd1Xi8Ehh9Jwc1JZloi153Sjb++u5zvbiimf8dWfpckInLEGntEUBX6pT4O+Jtz7kEgPXJlRY8bTj2GjKR47pu61O9SRES+ksYGQYmZ/RSv2+jroZ49R3jT3papVXKQb53enXcWb+GTtTv8LkdE5Ig1NgjGA+V41xNswusBdG/Eqooy157clay0BP40dYnfpYiIHLFGBUFo5/8M0MrMzgXKnHNPR7SyKJKaGM93RvTgw+Xb+GjFVr/LERE5Io0dYuIy4GPgUuAyYLaZXRLJwqLNlSd0JicjiT9NXUoUdXwSEWl009DP8K4huMY59w28cYT+X+TKij5JwQDfH9mDuWt2MH1JVFwKISICND4I4pxzW+pMbzuC98aMS4/vRKe2yfxx6hIdFYhI1GjszvxNM5tiZtea2bXA6xx0fYBAQnwct47sxYINu3hz/ia/yxERaZTGniz+MTARGBR6THTO/SSShUWrC4bk0j07lfveWkp1jY4KRKT5a3TzjnPuRefcbaHHS5EsKpoF4ozbzu7Nsi2lvPr5wWPsiYg0Pw0GgZmVmNmueh4lZrarqYqMNmMG5NCvQwZ/nLKU3eVVfpcjItKgBoPAOZfunMuo55HunMto6L2xLC7OuHNcfzYU7+XuNxf7XY6ISIPU8ydC8ru25dqTu/L0zDXMXLHN73JERA5JQRBBP/5abzq3TeEnL37Bngo1EYlI86QgiKCUhHjuuWQQa7fv4d4pGodIRJonBUGEnXhMJt84qQtPfrSaOau3+12OiMiXKAiawE9G9yG3dTK3v/AFeyuq/S5HROQACoImkJoYzz0XD2LV1t3c95aaiESkeVEQNJGTe2Tx9RM68+gHq5i7RjewEZHmQ0HQhH46pg8dMpK4/YXPKatUE5GINA8KgiaUnhTkrosHsaJoN395e5nf5YiIABEOAjMbbWZLzGy5md1xiHUuM7OFZrbAzP4dyXqag9N6ZTM+vxMT31vBZ+t2+l2OiEjkgsDMAsCDwBigH3CFmfU7aJ2ewE+B4c65/sCtkaqnOfnZuX1pn5HEj5//nPIqNRGJiL8ieUQwDFjunFvpnKsAngPGHbTOjcCDzrkdAAfd/KbFykgK8vuLBrJsSyl/fWe53+WISIyLZBDkAuvqTBeG5tXVC+hlZh+a2SwzG13fhsxsgpkVmFlBUVHLuA3kGb3bcfFxeTw0YwXzCov9LkdEYpjfJ4vjgZ7ACOAK4B9m1vrglZxzE51z+c65/Ozs7CYuMXJ+cW4/MlMT+PELn1NRVeN3OSISoyIZBOuBTnWm80Lz6ioEXnXOVTrnVgFL8YIhJrRKCfL7CweyeFMJD05TE5GI+COSQTAH6Glm3cwsAbgcePWgdV7GOxrAzLLwmopWRrCmZuesfu25YHBHHpy2nAUb1EQkIk0vYkHgnKsCbgKmAIuASc65BWZ2p5mdH1ptCrDNzBYC04AfO+dibvD+X57Xn9YpCfz4+S+orFYTkYg0LXMuum6wnp+f7woKCvwuI+zenL+Jb/9rLred3YubR8ZM65iINBEzm+ucy69vmd8niyVk9IAczh3Ugb++u4zFm3Q7aBFpOgqCZuTX5/cnIynIj5//gio1EYlIE1EQNCOZaYncOW4A89YX88h7MXXOXER8pCBoZs4Z1IExA3K4/+1lLNtc4nc5IhIDFATN0J3jBpCaGOBHL6iJSEQiT0HQDGWnJ/Kr8/vz+bqdPPbBKr/LEZEWTkHQTJ1/bEdG9WvPn95ayoqiUr/LEZEWTEHQTJkZv71wAMnBALe/8AXVNdF1vYeIRA8FQTPWLj2JX57Xj7lrdvDEh2oiEpHIUBA0cxcOyWVkn3b8ceoSVm/d7Xc5ItICKQiaOTPjdxcOJBiI4/YXvqBGTUQiEmYKgiiQ0yqJ/3duPz5evZ2nZ672uxwRaWEUBFHi0uPzOL1XNne/uYS12/b4XY6ItCAKgihhZvzhooEE4ozbX/xcTUQiEjYKgijSsXUyPzunL7NWbueZj9f6XY6ItBAKgihz+dBOnNIji7smL2LddjURicjRUxBEGTPjrosHAvDT/84j2m4sJCLNj4IgCuW1SeGnY/vywfKtPDdnnd/liEiUUxBEqa8P68xJx2Tyu9cXsX7nXr/LEZEopiCIUnFxxt0XD6K6xqmJSESOioIginXOTOGOMX14b2kRz88t9LscEYlSEQ0CMxttZkvMbLmZ3dHAehebmTOz/EjW0xJdfWIXhnVry29eW8jGYjURiciRi1gQmFkAeBAYA/QDrjCzfvWslw7cAsyOVC0tWVyccc/Fg6iqdlz894+Yu2aH3yWJSJSJ5BHBMGC5c26lc64CeA4YV896vwHuBsoiWEuL1jUrlecmnEggYIx/ZCaPzFihK49FpNEiGQS5QN2+jYWhefuY2XFAJ+fc6xGsIyYc26k1r33/VM7q254/vLGYG54uYMfuCr/LEpEo4NvJYjOLA+4DftiIdSeYWYGZFRQVFUW+uCjVKjnIQ1cdx6/P788Hy7Yy9oH3KVi93e+yRKSZi2QQrAc61ZnOC82rlQ4MAKab2WrgRODV+k4YO+cmOufynXP52dnZESw5+pkZ15zclRe/czLBQBzjJ87ioelqKhKRQ4tkEMwBeppZNzNLAC4HXq1d6Jwrds5lOee6Oue6ArOA851zBRGsKWYMzGvFazefwuj+Odz95mKue2oO29VUJCL1iFgQOOeqgJuAKcAiYJJzboGZ3Wlm50fqc2W/jKQgf/v6EH4zrj8fLd/G2Pvf5+NVaioSkQNZtF2Rmp+f7woKdNBwpOavL+amf3/Cuh17ue3sXnzn9O7ExZnfZYlIEzGzuc65eq/V0pXFMWJAbiv+9/1TGDMgh3unLOHaJ+ewrbTc77JEpBlQEMSQ9KQgf71iCL+7cACzVm5j7APvM3vlNr/LEhGfKQhijJlx5QldeOm7J5OSEM8V/5jF395dpl5FIjFMQRCj+nf0morOHdSRP05dyjVPfMxWNRWJxCQFQQxLS4zn/ssH84eLBvLxqu2Mvf99Zq5QU5FIrFEQxDgz40nk0ikAAA2kSURBVIphnXn5e8NJS4znykdncf/by6hWU5FIzFAQCAB9O2Twv++fwrjBufz57aV84/HZFJWoqUgkFigIZJ/UxHjuu+xY7rl4EAWrdzD2gff5aPlWv8sSkQhTEMgBzIzLhnbi1ZtOISMpnisfm82f31qqpiKRFkxBIPXqnZPOqzedwoVDcrn/nWVc9ehstpTolhEiLZGCQA7JayoazL2XDOLTdTsYe//7fLBMTUUiLY2CQA7r0nyvqahNSgJXPz6b+6YuUVORSAuiIJBG6dU+nVduGs4lx+XxwLvLuWLiLN6cv4myymq/SxORo6TRR+WIvTi3kN9NXsT23RWkJgQ4s297zhmYw+m92pGcEPC7PBGpR0Ojj8Y3dTES/S4+Po9xgzsya+V2Xp+3kSkLNvG/zzeQkhDgjD7tOGdgB0b0ziYlQX9eItFARwRy1Kqqa/h41f5Q2FpaQXIwwBl9shk7sANn9mmnUBDxWUNHBAoCCavqGsfHq7Yzed5G3pi/ia2l5SQF4xjRqx1jB3VgZJ92pCYqFESamoJAfFFd45izen8oFJWUkxgfx+m9sjlnUAdG9m1PmkJBpEkoCMR31TWOuWt2hEJhI5t3lZMQCoWxA3MY2bc9GUlBv8sUabEUBNKs1NQ4Plm7g9fnbeSNeZvYtKuMhEAcp/XKYsyADpzVrz2tkhUKIuGkIJBmq6bG8em6HUyet4k35m1kQ3EZwYBxas9sxgzIYVS/HFqlKBREjpaCQKJCTY3js8KdTP7CO6ewfudeggFjeI8szuzTjv4dW9EnJ10nm0W+At+CwMxGA/cDAeBR59xdBy2/DbgBqAKKgOucc2sa2qaCIDY45/i8sJjJ8zYyed5GCnfsBcAMumWm0rdjBv06ZNCvYwb9O2SQnZ6ImflctUjz5UsQmFkAWAqcDRQCc4ArnHML66xzBjDbObfHzL4DjHDOjW9ouwqC2OOcY0NxGQs37PIeG4tZuHEX67bv3bdOVloCfUPB0K9DBv07ZtAtK41AnMJBBPy7sngYsNw5tzJUxHPAOGBfEDjnptVZfxZwVQTrkShlZuS2Tia3dTJn92u/b37x3koWb9zFwo27WBR6fuKD1VRU1wCQFIyjd87+I4d+HTLUtCRSj0j+i8gF1tWZLgROaGD964E36ltgZhOACQCdO3cOV30S5VolBznhmExOOCZz37zK6hpWFJXWOXrYxeR5G3n247WAmpZE6tMsfhqZ2VVAPnB6fcudcxOBieA1DTVhaRJlgoE4+uRk0Ccng4uO8+Y559hY27S00QuIeYXFvP7Fxn3vO7hpqXPbFHJaJZGdlkh8QIP0SssWySBYD3SqM50XmncAMzsL+BlwunNOd0uXsDMzOrZOpmPrZM6q07S0q6ySxRtLWLjBO+dwcNMSQJxBdnoiORlJtM9IIqdV6JHhPdqHXqu5SaJZJP965wA9zawbXgBcDny97gpmNgR4BBjtnNsSwVpEviQjKciwbm0Z1q3tvnm1TUvrd+xl064yNheXsbG4jE27yli9bTezVm5jV1nVl7aVnhTvhUOrUGBk1AmM0LzM1ATidPJamqGIBYFzrsrMbgKm4HUffdw5t8DM7gQKnHOvAvcCacDzofbZtc658yNVk8jh1G1aOpQ9FVVsCoXD5l1lbCouZ1OxFxybdpWzbPNWtpSUcfBN3IIBo136/oDwjjASaZeeRFIwjoT4OIKBOBICcQTjvefE2nnx+5fXzlOPKAkXXVAmEgFV1TVsLa3wwqHYC4yNxbXBsX9671Hc4S0QZ15oBIyE+AAJATsgMBJCYVL7XDuvdUqQ7tlpdM9Oo0e7NNpn6ER5LNCNaUSaWHwgbt/5hAPOlNXhnGNXWRVFJeVUVNVQUV1DZXXNvtcVVd6jdl5ldQ3ltetVOSqqq6msdodcv3ZeaXlVnW04tpaUU1K+v3krNSFA93Zp9MhOo3u7NLpnp9KjXRqd26aSEK8T5bFAQSDiEzOjVXKwyQfYc85RVFLO8qJSVmwpZUXRblYUlTJz5Tb+++n+/hyBOKNLZsoBRw/ds1Pp3i5NI8W2MAoCkRhjZrTLSKJdRhInd886YFlpeRWrinazvKiEFVu8gFhRVMr0JVuorN7fjNwuPdELiHapdY4k0ujQKknNTFFIQSAi+6QlxjMwrxUD81odML+quoZ1O/ayYktpnSOJUl79bMMBvahSEgKhI4jU0BFEGtnpiaQlxZOWGE96YpDUxICuzWhmFAQicljxgTi6ZaXSLSuVs9h/LYZzjq2lFawoKmV5KBxWFO1mzuodvPzZhkNuLzkYIC0pnvTE+H0hkRZ6vX9ecP90nfUykoL7XuscRngoCETkKzMzstMTyU5P5MQ6Q32A1812ZdFuduypoLSsipLyKkrLqigt9x4lta/LKiktr2Lt9j0HLKs+uP9tPRLi4w4Ik9TEeJKCARLj4+o8x5EYHzjsc+JhlgcD9pWavZxzVNU4qmscldU1oecDp6tqaqiqcVRVu9Bz3emaffO7ZaXSOyf9iGs4HAWBiERESkI8A3JbHX7FejjnKKusoaS8cn94HDJM9q9TUlbFrr2VlFVWU1FVQ1llNeV1nqsaES6HEmd8KWQCcVbvDnvfjjwUAOHy7dO7c8eYPmHbXi0FgYg0O2ZGckKA5IQA7cL4A7gq1AX34IA41HN5nemyyhrKqw58rnaOYJwRiPOu5wjE2b6L/eIDRnycER8X5z0Hap9t37S3fuj9cfW9P+6A7WSlJ4TvP0YdCgIRiRnxgTjiA3EaG+ogOtMiIhLjFAQiIjFOQSAiEuMUBCIiMU5BICIS4xQEIiIxTkEgIhLjFAQiIjEu6u5QZmZFwJqv+PYsYGsYy2luWvL303eLXi35+0XTd+vinMuub0HUBcHRMLOCQ92qrSVoyd9P3y16teTv11K+m5qGRERinIJARCTGxVoQTPS7gAhryd9P3y16teTv1yK+W0ydIxARkS+LtSMCERE5iIJARCTGxUwQmNloM1tiZsvN7A6/6wkXM+tkZtPMbKGZLTCzW/yuKdzMLGBmn5rZa37XEm5m1trMXjCzxWa2yMxO8rumcDGzH4T+Jueb2bNmluR3TUfDzB43sy1mNr/OvLZm9paZLQs9t/Gzxq8qJoLAzALAg8AYoB9whZn187eqsKkCfuic6wecCHyvBX23WrcAi/wuIkLuB950zvUBjqWFfE8zywVuBvKdcwOAAHC5v1UdtSeB0QfNuwN4xznXE3gnNB11YiIIgGHAcufcSudcBfAcMM7nmsLCObfROfdJ6HUJ3o4k19+qwsfM8oBzgEf9riXczKwVcBrwGIBzrsI5t9PfqsIqHkg2s3ggBdjgcz1HxTn3HrD9oNnjgKdCr58CLmjSosIkVoIgF1hXZ7qQFrSzrGVmXYEhwGx/KwmrvwC3AzV+FxIB3YAi4IlQ09ejZpbqd1Hh4JxbD/wRWAtsBIqdc1P9rSoi2jvnNoZebwLa+1nMVxUrQdDimVka8CJwq3Nul9/1hIOZnQtscc7N9buWCIkHjgMecs4NAXYTpU0LBwu1lY/DC7uOQKqZXeVvVZHlvL74UdkfP1aCYD3Qqc50Xmhei2BmQbwQeMY591+/6wmj4cD5ZrYarznvTDP7l78lhVUhUOicqz2CewEvGFqCs4BVzrki51wl8F/gZJ9rioTNZtYBIPS8xed6vpJYCYI5QE8z62ZmCXgnrV71uaawMDPDa2Ne5Jy7z+96wsk591PnXJ5zrive/7N3nXMt5lelc24TsM7MeodmjQQW+lhSOK0FTjSzlNDf6EhayInwg7wKXBN6fQ3wio+1fGXxfhfQFJxzVWZ2EzAFr/fC4865BT6XFS7DgauBeWb2WWje/znnJvtYkzTe94FnQj9QVgLf9LmesHDOzTazF4BP8Hq2fUqUD8dgZs8CI4AsMysEfgncBUwys+vxhse/zL8KvzoNMSEiEuNipWlIREQOQUEgIhLjFAQiIjFOQSAiEuMUBCIiMU5BIBJhZjaiJY6cKi2HgkBEJMYpCERCzOwqM/vYzD4zs0dC90EoNbM/h8bVf8fMskPrDjazWWb2hZm9VDsOvZn1MLO3zexzM/vEzLqHNp9W574Dz4SutsXM7grdS+ILM/ujT19dYpyCQAQws77AeGC4c24wUA1cCaQCBc65/sAMvKtJAZ4GfuKcGwTMqzP/GeBB59yxeGPr1I5MOQS4Fe9+GMcAw80sE7gQ6B/azm8j+y1F6qcgEPGMBI4H5oSG6hiJt8OuAf4TWudfwCmh+wi0ds7NCM1/CjjNzNKBXOfcSwDOuTLn3J7QOh875wqdczXAZ0BXoBgoAx4zs4uA2nVFmpSCQMRjwFPOucGhR2/n3K/qWe+rjslSXud1NRDvnKvCu2nSC8C5wJtfcdsiR0VBIOJ5B7jEzNrBvnvRdsH7N3JJaJ2vAx8454qBHWZ2amj+1cCM0B3iCs3sgtA2Es0s5VAfGLqHRKvQAIE/wLtVpUiTi4nRR0UOxzm30Mx+Dkw1szigEvge3s1ihoWWbcE7jwDekMMPh3b0dUcNvRp4xMzuDG3j0gY+Nh14JXRTdwNuC/PXEmkUjT4q0gAzK3XOpfldh0gkqWlIRCTG6YhARCTG6YhARCTGKQhERGKcgkBEJMYpCEREYpyCQEQkxv1/Q/1ZiQL8W9IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSaHVGOwdyNi"
      },
      "source": [
        "I would do an exaustive gridsearch on my RNN building function to test out LSTM's and GRU's of varying parameters and sizes. however fixing class imbalances didn't help and I don't want to waste compute. I will instead use glove embeddings rather than creating my own embedding layer. I will also try a traditional machine learning approach later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUFOQvyoovhk"
      },
      "source": [
        "tokens = []\r\n",
        "for num in ix:\r\n",
        "  tokens.append(df['tweet'][num].split(' '))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0QWQIxxembK"
      },
      "source": [
        "from gensim.models import Word2Vec, keyedvectors\r\n",
        "\r\n",
        "W = Word2Vec(tokens, size=33, window=3, min_count=1, workers=4)\r\n",
        "W.train(sentences=tokens, total_examples=W.corpus_count, epochs=10)\r\n",
        "wv = W.wv"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H75VStkbnITT"
      },
      "source": [
        "e = W.wv.get_keras_embedding(train_embeddings=False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC_6eCEkoYnW",
        "outputId": "d19f1a97-5fcd-48c4-e5cd-14afd5a7b015"
      },
      "source": [
        "from keras.metrics import Recall\r\n",
        "\r\n",
        "model = models.Sequential()\r\n",
        "model.add(e)\r\n",
        "model.add(layers.Bidirectional(layers.LSTM(128)))\r\n",
        "model.add(layers.Dense(64, activation='relu'))\r\n",
        "model.add(layers.Dense(32, activation='relu'))\r\n",
        "model.add(layers.Dense(3, activation='softmax'))\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "early_stopping = [EarlyStopping(patience=10), ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5')]\r\n",
        "\r\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2, callbacks=early_stopping, class_weight=weights_dict)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 1.0752 - accuracy: 0.3676 - val_loss: 1.2630 - val_accuracy: 0.2231\n",
            "Epoch 2/50\n",
            "101/101 [==============================] - 1s 9ms/step - loss: 1.0432 - accuracy: 0.3952 - val_loss: 1.0083 - val_accuracy: 0.4549\n",
            "Epoch 3/50\n",
            "101/101 [==============================] - 1s 10ms/step - loss: 1.0143 - accuracy: 0.4124 - val_loss: 1.0885 - val_accuracy: 0.4027\n",
            "Epoch 4/50\n",
            "101/101 [==============================] - 1s 10ms/step - loss: 0.9898 - accuracy: 0.4631 - val_loss: 1.0881 - val_accuracy: 0.4288\n",
            "Epoch 5/50\n",
            "101/101 [==============================] - 1s 9ms/step - loss: 0.9609 - accuracy: 0.4629 - val_loss: 0.9884 - val_accuracy: 0.4897\n",
            "Epoch 6/50\n",
            "101/101 [==============================] - 1s 10ms/step - loss: 0.9206 - accuracy: 0.4923 - val_loss: 1.0961 - val_accuracy: 0.3679\n",
            "Epoch 7/50\n",
            "101/101 [==============================] - 1s 10ms/step - loss: 0.8985 - accuracy: 0.4862 - val_loss: 0.9297 - val_accuracy: 0.5071\n",
            "Epoch 8/50\n",
            "101/101 [==============================] - 1s 10ms/step - loss: 0.8477 - accuracy: 0.5181 - val_loss: 0.9167 - val_accuracy: 0.5320\n",
            "Epoch 9/50\n",
            "101/101 [==============================] - 1s 9ms/step - loss: 0.7885 - accuracy: 0.5430 - val_loss: 0.9411 - val_accuracy: 0.5295\n",
            "Epoch 10/50\n",
            "101/101 [==============================] - 1s 9ms/step - loss: 0.7408 - accuracy: 0.5661 - val_loss: 0.9112 - val_accuracy: 0.5482\n",
            "Epoch 11/50\n",
            "101/101 [==============================] - 1s 10ms/step - loss: 0.6792 - accuracy: 0.5952 - val_loss: 1.0327 - val_accuracy: 0.4866\n",
            "Epoch 12/50\n",
            "101/101 [==============================] - 1s 10ms/step - loss: 0.6273 - accuracy: 0.6185 - val_loss: 0.9786 - val_accuracy: 0.5301\n",
            "Epoch 13/50\n",
            "101/101 [==============================] - 1s 9ms/step - loss: 0.5930 - accuracy: 0.6376 - val_loss: 1.0867 - val_accuracy: 0.4817\n",
            "Epoch 14/50\n",
            "101/101 [==============================] - 1s 9ms/step - loss: 0.5409 - accuracy: 0.6630 - val_loss: 1.0300 - val_accuracy: 0.5333\n",
            "Epoch 15/50\n",
            "101/101 [==============================] - 1s 10ms/step - loss: 0.5039 - accuracy: 0.6804 - val_loss: 1.1068 - val_accuracy: 0.5339\n",
            "Epoch 16/50\n",
            "101/101 [==============================] - 1s 9ms/step - loss: 0.4895 - accuracy: 0.6921 - val_loss: 1.0426 - val_accuracy: 0.5606\n",
            "Epoch 17/50\n",
            "101/101 [==============================] - 1s 10ms/step - loss: 0.4613 - accuracy: 0.7067 - val_loss: 1.0864 - val_accuracy: 0.5544\n",
            "Epoch 18/50\n",
            "101/101 [==============================] - 1s 10ms/step - loss: 0.3993 - accuracy: 0.7457 - val_loss: 1.1821 - val_accuracy: 0.5457\n",
            "Epoch 19/50\n",
            "101/101 [==============================] - 1s 9ms/step - loss: 0.3702 - accuracy: 0.7634 - val_loss: 1.2058 - val_accuracy: 0.5370\n",
            "Epoch 20/50\n",
            "101/101 [==============================] - 1s 9ms/step - loss: 0.3538 - accuracy: 0.7693 - val_loss: 1.1736 - val_accuracy: 0.5823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "Kn52RekKt81l",
        "outputId": "068be2b8-5a6c-4bc8-dd53-34102cdfd163"
      },
      "source": [
        "df"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>emotion_in_tweet_is_directed_at</th>\n",
              "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
              "      <td>iPhone</td>\n",
              "      <td>Negative emotion</td>\n",
              "      <td>wesley83 i have a 3g iphone after 3 hrs tweeti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
              "      <td>iPad or iPhone App</td>\n",
              "      <td>Positive emotion</td>\n",
              "      <td>jessedee know about fludapp  awesome ipad/ipho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
              "      <td>iPad</td>\n",
              "      <td>Positive emotion</td>\n",
              "      <td>swonderlin can not wait for #ipad 2 also they ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
              "      <td>iPad or iPhone App</td>\n",
              "      <td>Negative emotion</td>\n",
              "      <td>sxsw i hope this year's festival isn't as cras...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
              "      <td>Google</td>\n",
              "      <td>Positive emotion</td>\n",
              "      <td>sxtxstate great stuff on fri #sxsw marissa may...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9088</th>\n",
              "      <td>Ipad everywhere. #SXSW {link}</td>\n",
              "      <td>iPad</td>\n",
              "      <td>Positive emotion</td>\n",
              "      <td>ipad everywhere #sxsw {link}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9089</th>\n",
              "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No emotion toward brand or product</td>\n",
              "      <td>wave buzz rt mention we interrupt your regular...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9090</th>\n",
              "      <td>Google's Zeiger, a physician never reported po...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No emotion toward brand or product</td>\n",
              "      <td>google's zeiger a physician never reported pot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9091</th>\n",
              "      <td>Some Verizon iPhone customers complained their...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No emotion toward brand or product</td>\n",
              "      <td>some verizon iphone customers complained their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9092</th>\n",
              "      <td>�ϡ�����_��ʋ�΋�ҋ�������⋁_��������_���RT @mentio...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No emotion toward brand or product</td>\n",
              "      <td>�ϡ�����_��ʋ�΋�ҋ�������⋁_��������_���rt mention...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8936 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             tweet_text  ...                                              tweet\n",
              "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...  ...  wesley83 i have a 3g iphone after 3 hrs tweeti...\n",
              "1     @jessedee Know about @fludapp ? Awesome iPad/i...  ...  jessedee know about fludapp  awesome ipad/ipho...\n",
              "2     @swonderlin Can not wait for #iPad 2 also. The...  ...  swonderlin can not wait for #ipad 2 also they ...\n",
              "3     @sxsw I hope this year's festival isn't as cra...  ...  sxsw i hope this year's festival isn't as cras...\n",
              "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...  ...  sxtxstate great stuff on fri #sxsw marissa may...\n",
              "...                                                 ...  ...                                                ...\n",
              "9088                      Ipad everywhere. #SXSW {link}  ...                       ipad everywhere #sxsw {link}\n",
              "9089  Wave, buzz... RT @mention We interrupt your re...  ...  wave buzz rt mention we interrupt your regular...\n",
              "9090  Google's Zeiger, a physician never reported po...  ...  google's zeiger a physician never reported pot...\n",
              "9091  Some Verizon iPhone customers complained their...  ...  some verizon iphone customers complained their...\n",
              "9092  �ϡ�����_��ʋ�΋�ҋ�������⋁_��������_���RT @mentio...  ...  �ϡ�����_��ʋ�΋�ҋ�������⋁_��������_���rt mention...\n",
              "\n",
              "[8936 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K79F7g6VQkAh",
        "outputId": "cd180672-bf79-44ae-898f-159175189ed1"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEo63KwpRGiy"
      },
      "source": [
        "stopwords_list = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQrLc68ar79A"
      },
      "source": [
        "vocab = []\r\n",
        "for token in tokens:\r\n",
        "  if token not in stopwords_list:\r\n",
        "    vocab += token\r\n",
        "\r\n",
        "wrds = list(set(vocab))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Dhuvdib0wAj"
      },
      "source": [
        "glove = {}\r\n",
        "for w in wrds:\r\n",
        "  glove[w] = wv.get_vector(w)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZu-hrZs1oZC"
      },
      "source": [
        "class W2vVectorizer(object):\r\n",
        "\r\n",
        "  def __init__(self, w2v):\r\n",
        "    self.w2v = w2v\r\n",
        "    if len(w2v) == 0:\r\n",
        "      self.dimensions = 0\r\n",
        "    else:\r\n",
        "      self.dimensions = len(w2v[next(iter(glove))])\r\n",
        "\r\n",
        "  def fit(self, X, y):\r\n",
        "    return self\r\n",
        "\r\n",
        "  def transform(self, X):\r\n",
        "    return np.array([\r\n",
        "        np.mean([self.w2v[w] for w in words if w in self.w2v]\r\n",
        "            or [np.zeros(self.dimensions)], axis=0) for words in X])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O362_8VO5OLz"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "\r\n",
        "rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\r\n",
        "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\r\n",
        "svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\r\n",
        "                ('Support Vector Machine', SVC())])\r\n",
        "lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\r\n",
        "              ('Logistic Regression', LogisticRegression())])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhvOVq-M5tZ0"
      },
      "source": [
        "models = [('Random Forest', rf),\r\n",
        "          ('Support Vector Machine', svc),\r\n",
        "          ('Logistic Regression', lr)]\r\n",
        "\r\n",
        "w2v = W2vVectorizer(glove)\r\n",
        "\r\n",
        "data2 = pd.Series(tokens)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3eDJPkC7l49"
      },
      "source": [
        "scores = [(name, cross_val_score(model, data2, y, cv=2, n_jobs=-1).mean()) for name, model in models]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQp3kBRK715L",
        "outputId": "83b4ece4-a6e7-4843-b1de-1ae851c00dd1"
      },
      "source": [
        "scores"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Random Forest', 0.534579230080573),\n",
              " ('Support Vector Machine', nan),\n",
              " ('Logistic Regression', nan)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fednv9Af-29n"
      },
      "source": [
        "Trying a different approach! I one hot encoded each tweet for all of the words in the corpus. I will use PCA to compress the very sparse data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajS6ZqIl8G7i"
      },
      "source": [
        "X_oh, y_oh = dh.ModelReadyText2(df, 'tweet', 'is_there_an_emotion_directed_at_a_brand_or_product', len(glove))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8peGEsO8yKX"
      },
      "source": [
        "classes = []\r\n",
        "for arr in y_oh:\r\n",
        "  classes.append(arr.argmax())"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "ZXLgid4n9MI7",
        "outputId": "1a8cfd8c-28dc-490e-e859-6689b21a6b03"
      },
      "source": [
        "df2 = pd.DataFrame(X_oh)\r\n",
        "df2['target'] = classes\r\n",
        "df2"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>12319</th>\n",
              "      <th>12320</th>\n",
              "      <th>12321</th>\n",
              "      <th>12322</th>\n",
              "      <th>12323</th>\n",
              "      <th>12324</th>\n",
              "      <th>12325</th>\n",
              "      <th>12326</th>\n",
              "      <th>12327</th>\n",
              "      <th>12328</th>\n",
              "      <th>12329</th>\n",
              "      <th>12330</th>\n",
              "      <th>12331</th>\n",
              "      <th>12332</th>\n",
              "      <th>12333</th>\n",
              "      <th>12334</th>\n",
              "      <th>12335</th>\n",
              "      <th>12336</th>\n",
              "      <th>12337</th>\n",
              "      <th>12338</th>\n",
              "      <th>12339</th>\n",
              "      <th>12340</th>\n",
              "      <th>12341</th>\n",
              "      <th>12342</th>\n",
              "      <th>12343</th>\n",
              "      <th>12344</th>\n",
              "      <th>12345</th>\n",
              "      <th>12346</th>\n",
              "      <th>12347</th>\n",
              "      <th>12348</th>\n",
              "      <th>12349</th>\n",
              "      <th>12350</th>\n",
              "      <th>12351</th>\n",
              "      <th>12352</th>\n",
              "      <th>12353</th>\n",
              "      <th>12354</th>\n",
              "      <th>12355</th>\n",
              "      <th>12356</th>\n",
              "      <th>12357</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8931</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8932</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8933</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8934</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8935</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8936 rows × 12359 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0    1    2    3    4    5  ...  12353  12354  12355  12356  12357  target\n",
              "0     0.0  1.0  0.0  0.0  0.0  1.0  ...    0.0    0.0    0.0    0.0    0.0       0\n",
              "1     0.0  1.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       2\n",
              "2     0.0  1.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       2\n",
              "3     0.0  1.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       0\n",
              "4     0.0  1.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       2\n",
              "...   ...  ...  ...  ...  ...  ...  ...    ...    ...    ...    ...    ...     ...\n",
              "8931  0.0  1.0  0.0  0.0  1.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       2\n",
              "8932  0.0  1.0  1.0  0.0  1.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       1\n",
              "8933  0.0  1.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       1\n",
              "8934  0.0  1.0  0.0  1.0  0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       1\n",
              "8935  0.0  1.0  1.0  0.0  1.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       1\n",
              "\n",
              "[8936 rows x 12359 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDhgJjR4_QgU"
      },
      "source": [
        "from sklearn.decomposition import PCA\r\n",
        "\r\n",
        "pca = PCA(n_components=290)\r\n",
        "\r\n",
        "X = df2.drop(['target'], axis='columns')\r\n",
        "\r\n",
        "df3 = pd.DataFrame(pca.fit_transform(X))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLes4KIBSKPU"
      },
      "source": [
        "df3['target'] = classes"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwm7fEnpAgPq",
        "outputId": "08e96bbe-a39d-4c58-c6f3-341ba2b0d979"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.svm import SVC, SVR\r\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\r\n",
        "from sklearn.neighbors import  KNeighborsClassifier, KNeighborsRegressor\r\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\r\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\r\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, GradientBoostingRegressor\r\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\r\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, plot_confusion_matrix\r\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\r\n",
        "from imblearn.over_sampling import SMOTE\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from xgboost import XGBClassifier, XGBRegressor\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from sklearn.feature_selection import SelectKBest\r\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\r\n",
        "\r\n",
        "class MachineLearning:\r\n",
        "    \r\n",
        "    def CompareClassifiers(self, X, y):\r\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\r\n",
        "        n = len(np.unique(y))\r\n",
        "        if n == 2:\r\n",
        "            methods = [KNeighborsClassifier(), GaussianNB(), DecisionTreeClassifier(), RandomForestClassifier(),\r\n",
        "                      AdaBoostClassifier(), GradientBoostingClassifier(), XGBClassifier(), LogisticRegression(),\r\n",
        "                      SVC()]\r\n",
        "            strs = ['KNN', 'NB', 'DT', 'RF', 'AB', 'GB', 'XGB', 'Log', 'SVM']\r\n",
        "        else:\r\n",
        "            methods = [KNeighborsClassifier(), DecisionTreeClassifier(), RandomForestClassifier(),\r\n",
        "                       AdaBoostClassifier(), GradientBoostingClassifier(), XGBClassifier(), SVC()]\r\n",
        "            strs = ['KNN', 'DT', 'RF', 'AB', 'GB', 'XGB', 'SVM']\r\n",
        "        train_acc = []\r\n",
        "        test_acc = []\r\n",
        "        for i in range(len(methods)):\r\n",
        "            clf = methods[i].fit(X_train, y_train)\r\n",
        "            train_acc.append(clf.score(X_train, y_train))\r\n",
        "            test_acc.append(clf.score(X_test, y_test))\r\n",
        "        c1 = pd.DataFrame(strs)\r\n",
        "        c2 = pd.DataFrame(train_acc)\r\n",
        "        c3 = pd.DataFrame(test_acc)\r\n",
        "        results = pd.concat([c1, c2, c3], axis='columns')\r\n",
        "        results.columns = ['Model', 'train_acc', 'test_acc']\r\n",
        "        return results\r\n",
        "    \r\n",
        "    def CompareRegressors(self, X, y):\r\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\r\n",
        "        n = len(np.unique(y))\r\n",
        "        methods = [LinearRegression(), KNeighborsRegressor(), DecisionTreeRegressor(), RandomForestRegressor(),\r\n",
        "                       AdaBoostRegressor(), GradientBoostingRegressor(), XGBRegressor(), SVR()]\r\n",
        "        strs = ['Lin', 'KNN', 'DT', 'RF', 'AB', 'GB', 'XGB', 'SVM']\r\n",
        "        train_acc = []\r\n",
        "        test_acc = []\r\n",
        "        for i in range(len(methods)):\r\n",
        "            reg = methods[i].fit(X_train, y_train)\r\n",
        "            train_acc.append(reg.score(X_train, y_train))\r\n",
        "            test_acc.append(reg.score(X_test, y_test))\r\n",
        "        c1 = pd.DataFrame(strs)\r\n",
        "        c2 = pd.DataFrame(train_acc)\r\n",
        "        c3 = pd.DataFrame(test_acc)\r\n",
        "        results = pd.concat([c1, c2, c3], axis='columns')\r\n",
        "        results.columns = ['Model', 'train_acc', 'test_acc']\r\n",
        "        return results\r\n",
        "    \r\n",
        "    def Optimize(self, model, parameters, X, y, metric='accuracy'):\r\n",
        "        try:\r\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\r\n",
        "            return GridSearchCV(model, parameters, cv=5, scoring=metric, n_jobs=-1, verbose=2).fit(X_train, y_train).best_estimator_\r\n",
        "        except:\r\n",
        "            return GridSearchCV(model, parameters, cv=5, n_jobs=-1, verbose=2).fit(X_train, y_train).best_estimator_\r\n",
        "        \r\n",
        "    def PipeIt(self, scaler, model, X, y):\r\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\r\n",
        "        pipe = Pipeline([('scaler', scaler), ('model', model)]).fit(X_train, y_train)\r\n",
        "        return \"Training: {}, Validation: {}\".format(pipe.score(X_train, y_train), pipe.score(X_test, y_test))\r\n",
        "\r\n",
        "\r\n",
        "class DataHelper:\r\n",
        "    \r\n",
        "    def ScaleData(self, strategy, y_var, data):\r\n",
        "        X = data.drop([y_var], axis='columns')\r\n",
        "        if strategy == 'minmax':\r\n",
        "            return MinMaxScaler().fit(X).transform(X)\r\n",
        "        if strategy == 'standard':\r\n",
        "            return StandardScaler().fit(X).transform(X)\r\n",
        "        if strategy == 'mean':\r\n",
        "            for col in X.columns:\r\n",
        "                X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col]))\r\n",
        "            return X\r\n",
        "        \r\n",
        "    def HoldOut(self, data):\r\n",
        "        train, test = train_test_split(data, test_size=0.1)\r\n",
        "        return train, test\r\n",
        "    \r\n",
        "    def MakeNewDF(self, X, y, k):\r\n",
        "        selector = SelectKBest(k=k).fit(X, y)\r\n",
        "        mask = selector.get_support()\r\n",
        "        selected = []\r\n",
        "        for i in range(len(mask)):\r\n",
        "            if mask[i] == True:\r\n",
        "                selected.append(X.columns[i])\r\n",
        "        df = pd.DataFrame(selector.transform(X))\r\n",
        "        df.columns = selected\r\n",
        "        return df\r\n",
        "    \r\n",
        "    def VifIt(self, X):\r\n",
        "        vif = pd.Series([variance_inflation_factor(X.values, i) \r\n",
        "               for i in range(X.shape[1])], \r\n",
        "              index=X.columns)\r\n",
        "        return vif\r\n",
        "        \r\n",
        "    \r\n",
        "class Evaluater:\r\n",
        "    \r\n",
        "    def ScoreModel(self, model, X, y):\r\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\r\n",
        "        algo = model.fit(X_train, y_train)\r\n",
        "        return 'Training: {}, Validation: {}'.format(algo.score(X_train, y_train), algo.score(X_test, y_test))\r\n",
        "    \r\n",
        "    def BuildConfusion(self, fitted_model,  Xval, yval, cmap='plasma'):\r\n",
        "        cm = plot_confusion_matrix(fitted_model, Xval, yval, cmap=cmap)\r\n",
        "        return cm\r\n",
        "    \r\n",
        "    def BuildTree(self, tree):\r\n",
        "        try:\r\n",
        "            return plot_tree(tree)\r\n",
        "        except:\r\n",
        "            return 'Please pass a tree class'\r\n",
        "    \r\n",
        "    def GetCoefficients(self, model, X, y):\r\n",
        "        try:\r\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\r\n",
        "            model.fit(X_train, y_train)\r\n",
        "            return 'coefficients: {}'.format(model.coef_)\r\n",
        "        except:\r\n",
        "            return 'Please pass LinearRegression, LogisticRegression, or an SVM with a linear kernel'\r\n",
        "        \r\n",
        "    def GetImportance(self, model, X, y):\r\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\r\n",
        "        model.fit(X_train, y_train)\r\n",
        "        try:\r\n",
        "            FI = model.feature_importances_\r\n",
        "            n_features = X_train.shape[1]\r\n",
        "            plt.figure(figsize=(8,8))\r\n",
        "            plt.barh(range(n_features), FI, align='center') \r\n",
        "            plt.yticks(np.arange(n_features), X_train.columns.values) \r\n",
        "            plt.xlabel('Feature importance')\r\n",
        "            plt.ylabel('Feature')\r\n",
        "        except:\r\n",
        "            return 'Please pass an ensemble class'\r\n",
        "        \r\n",
        "    def AUC(self, model, Xval, yval):\r\n",
        "        pred = model.predict(Xval)\r\n",
        "        fpr, tpr, threshold = roc_curve(yval, pred)\r\n",
        "        return auc(fpr, tpr)\r\n",
        "    \r\n",
        "ml = MachineLearning()\r\n",
        "dh2 = DataHelper()\r\n",
        "ev = Evaluater()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYwS-HyaBDrd"
      },
      "source": [
        "X_train = dh2.HoldOut(df3)[0].drop(['target'], axis='columns')\r\n",
        "y_train = dh2.HoldOut(df3)[0]['target']\r\n",
        "X_test = dh2.HoldOut(df3)[1].drop(['target'], axis='columns')\r\n",
        "y_test = dh2.HoldOut(df3)[1]['target']"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yRcuEfOExUm",
        "outputId": "0b9aed24-ec61-4d9a-f849-a78460f96617"
      },
      "source": [
        "model = XGBClassifier()\r\n",
        "\r\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='multi:softprob', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEnMV5-UTx8c",
        "outputId": "6f2a7451-5898-419a-970a-a84c0569b583"
      },
      "source": [
        "model.score(X_test, y_test)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5928411633109619"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnfzoIalUTsw",
        "outputId": "8e8d3ca0-904a-489c-8f2e-110bd4485e5b"
      },
      "source": [
        "model.score(X_train, y_train)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6457348918179557"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "q4gC2ld8UbYF",
        "outputId": "a18db8e3-b7a5-473b-becb-043415860aef"
      },
      "source": [
        "ev.BuildConfusion(model, X_test, y_test)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f4130621710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVsz_tAoVMok",
        "outputId": "918608a2-0765-4ece-ca2a-1722c87626d7"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\r\n",
        "sm = SMOTE()\r\n",
        "X_res, y_res = sm.fit_resample(X_train, y_train)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsFoifwiWgY7",
        "outputId": "92b586a7-f737-44e7-82fd-54640da0ee58"
      },
      "source": [
        "xgb = XGBClassifier()\r\n",
        "xgb.fit(X_res, y_res)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='multi:softprob', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKWNuaMXXifj",
        "outputId": "696cbe97-1563-4f86-89c2-ef8805e4ccca"
      },
      "source": [
        "xgb.score(X_res, y_res)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7059227232173495"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRUNhC42XsAa",
        "outputId": "7f020294-7054-432f-ae74-aff4543963fa"
      },
      "source": [
        "xgb.score(np.array(X_test), np.array(y_test))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4395973154362416"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "8YYzRvf_YN0J",
        "outputId": "33f9ec91-7b64-4b19-819a-4ebe9fc4c297"
      },
      "source": [
        "ev.BuildConfusion(xgb, np.array(X_test), np.array(y_test))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f416e273fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8dzlsNel17n"
      },
      "source": [
        "After trying various RNN's and gradient boosted forests trained on word embeddings, and trained on a PCA transformed one hot encoded vectors. I've concluded that the RNN saved as 'bid.h5' was the best. Although it was overfit it got 94% accuracy. I will now access random tweets outside of the dataset via the twitter API and see how the model classifies the tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_MgBw48nHfo"
      },
      "source": [
        "from keras import models\r\n",
        "\r\n",
        "model = models.load_model('bid.h5')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "tdd_Xj6gsGP2",
        "outputId": "4e729e4f-787a-48d4-fdbd-090f84529d87"
      },
      "source": [
        "from google.colab import files\r\n",
        "path_to_file = list(files.upload().keys())[0]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dc66c631-83d4-46b4-ac1b-53aae745112b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dc66c631-83d4-46b4-ac1b-53aae745112b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ssh-twitter.txt to ssh-twitter.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsM_rvzOntRe"
      },
      "source": [
        "import tweepy\r\n",
        "\r\n",
        "c1 = open('ssh-twitter.txt').readlines()[2].split(':')[1].replace('\\n', '').replace(' ', '')\r\n",
        "c2 = open('ssh-twitter.txt').readlines()[3].split(':')[1].replace('\\n', '').replace(' ', '')\r\n",
        "a1 = open('ssh-twitter.txt').readlines()[0].split(':')[1].replace('\\n', '').replace(' ', '')\r\n",
        "a2 = open('ssh-twitter.txt').readlines()[1].split(':')[1].replace('\\n', '').replace(' ', '')\r\n",
        "\r\n",
        "auth = tweepy.OAuthHandler(c1, c2)\r\n",
        "auth.set_access_token(a1, a2)\r\n",
        "\r\n",
        "def GetTweets(Topic):\r\n",
        "  api = tweepy.API(auth)\r\n",
        "  topic = api.search(Topic)\r\n",
        "  tweets = []\r\n",
        "  time = []\r\n",
        "  for i in range(len(topic)):\r\n",
        "    tweets.append(topic[i]._json['text'].replace('\\n', '').replace('.@', '').replace('.', '').replace('!', '').replace('@', '')\r\n",
        "    .replace('?', '').replace(':', '').replace(',', '').replace(';', '').lower())\r\n",
        "    time.append(topic[i]._json['created_at'])\r\n",
        "    res = pd.DataFrame(tweets)\r\n",
        "  res.columns = ['text']\r\n",
        "  res['time'] = time \r\n",
        "  X = dh.ModelReadyText1(res, 'text', 'time', 33)[0]\r\n",
        "  backup_dim = len(X), len(res)\r\n",
        "  sent = []\r\n",
        "  for arr in X:\r\n",
        "    sentiment = int(round(model.predict(arr).argmax()/len(arr)-1))\r\n",
        "    if sentiment == 0:\r\n",
        "      sent.append('Negative')\r\n",
        "    if sentiment == 1:\r\n",
        "      sent.append('Neutral')\r\n",
        "    if sentiment == 2:\r\n",
        "      sent.append('Positive')\r\n",
        "  res['sentiment'] = sent \r\n",
        "  results = res.drop(['time'], axis='columns')\r\n",
        "  return results"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weLIf7FHBdtf"
      },
      "source": [
        "With this GetTweets function all you have to do is enter a topic and you'll get a dataframe with tweets selected by the tweepy module and the sentiment that the model predicts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "4tcAwDW0BWrP",
        "outputId": "d2aadaa6-5e1a-4cd4-ba48-6a5f94fbd003"
      },
      "source": [
        "GetTweets('Covid')"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rt luiscarrillo66 descubren que el covid-19 ya...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rt shanabc04 les gens qui aiment pas l’album d...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>rt drericding covid killed christmas for our k...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>shekhargupta meher70090510 _yogendrayadav our ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rt thedailyshow a staten island bar owner who ...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>rt muyregiaa mis logros de este año1- no tuve ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>rt sandra_barba ahora me entero de que hay un ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>rt facesofcovid robert \"bobby\" sullivan 72 of ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>rt escritoslana llegué a diciembre sin tener c...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>rt partidopsuv día 269 de la lucha contra la c...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>#thursdayvibe #covid #math #physics today's ne...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>rt fertilizermkts #cdc no longer collecting #i...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>rt platinumlasher getting a covid test and my ...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>rt ministerieezk winkelen en boodschappen doen...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>rt carolinelucas i’m very glad that people sle...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text sentiment\n",
              "0   rt luiscarrillo66 descubren que el covid-19 ya...   Neutral\n",
              "1   rt shanabc04 les gens qui aiment pas l’album d...   Neutral\n",
              "2   rt drericding covid killed christmas for our k...   Neutral\n",
              "3   shekhargupta meher70090510 _yogendrayadav our ...  Positive\n",
              "4   rt thedailyshow a staten island bar owner who ...  Negative\n",
              "5   rt muyregiaa mis logros de este año1- no tuve ...   Neutral\n",
              "6   rt sandra_barba ahora me entero de que hay un ...   Neutral\n",
              "7   rt facesofcovid robert \"bobby\" sullivan 72 of ...  Positive\n",
              "8   rt escritoslana llegué a diciembre sin tener c...  Positive\n",
              "9   rt partidopsuv día 269 de la lucha contra la c...  Negative\n",
              "10  #thursdayvibe #covid #math #physics today's ne...   Neutral\n",
              "11  rt fertilizermkts #cdc no longer collecting #i...   Neutral\n",
              "12  rt platinumlasher getting a covid test and my ...  Negative\n",
              "13  rt ministerieezk winkelen en boodschappen doen...  Positive\n",
              "14  rt carolinelucas i’m very glad that people sle...  Positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    }
  ]
}